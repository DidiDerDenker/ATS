\babel@toc {ngerman}{}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Ablauf einer automatischen Zusammenfassung \cite {THA19}.\relax }}{1}{figure.caption.15}%
\contentsline {figure}{\numberline {1.2}{\ignorespaces Aufbau der Arbeit.\relax }}{4}{figure.caption.23}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Aufbau eines künstlichen Neurons \cite {MCC20}.\relax }}{7}{figure.caption.31}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces Aufbau eines MLP \cite [S.~388]{RAS19}.\relax }}{7}{figure.caption.33}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces Supervised Learning \cite [S.~3]{RAS19}.\relax }}{8}{figure.caption.34}%
\contentsline {figure}{\numberline {2.4}{\ignorespaces Typen von Generalisierungseffekten \cite {EDPOJ}.\relax }}{9}{figure.caption.35}%
\contentsline {figure}{\numberline {2.5}{\ignorespaces Encoder-Decoder-Architektur \cite [S.~375]{ZHA20}.\relax }}{10}{figure.caption.38}%
\contentsline {figure}{\numberline {2.6}{\ignorespaces Self-Attention \cite [S.~400]{ZHA20}.\relax }}{12}{figure.caption.41}%
\contentsline {figure}{\numberline {2.7}{\ignorespaces Multi-Head-Attention \cite [S.~400]{ZHA20}.\relax }}{13}{figure.caption.42}%
\contentsline {figure}{\numberline {2.8}{\ignorespaces Transformer-Architektur \cite [S.~3]{VAS17}.\relax }}{14}{figure.caption.44}%
\contentsline {figure}{\numberline {2.9}{\ignorespaces Konvergenzverhalten im Gradientenverfahren \cite [S.~429]{ZHA20}.\relax }}{15}{figure.caption.47}%
\contentsline {figure}{\numberline {2.10}{\ignorespaces Gradientenverfahren unter Einfluss eines Momentums \cite {CSNOJ}.\relax }}{17}{figure.caption.49}%
\contentsline {figure}{\numberline {2.11}{\ignorespaces Fine-Tuning vortrainierter Modelle \cite [S.~555]{ZHA20}.\relax }}{18}{figure.caption.51}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Tokenisierung eines beispielhaften Satzes.\relax }}{22}{figure.caption.60}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces Architektur und Funktionsweise von ELMo \cite {IRE18}.\relax }}{25}{figure.caption.69}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces Architektur von BERT mit MLM \cite [S.~3]{DEV19}.\relax }}{27}{figure.caption.74}%
\contentsline {figure}{\numberline {3.4}{\ignorespaces Repräsentation von Textdaten mithilfe von BERT \cite [S.~3]{DEV19}.\relax }}{28}{figure.caption.75}%
\contentsline {figure}{\numberline {3.5}{\ignorespaces BERT, GPT und BART im Vergleich \cite [S.~2]{LEW19}.\relax }}{30}{figure.caption.80}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Sequence-to-Sequence-Transformer-Modell mit BERT \cite {VON20}.\relax }}{36}{figure.caption.95}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {7.1}{\ignorespaces Loss in der SOTA-Reproduktion im Modellvergleich.\relax }}{47}{figure.caption.112}%
\contentsline {figure}{\numberline {7.2}{\ignorespaces Loss in der Adaption auf deutschen Daten im Modellvergleich.\relax }}{48}{figure.caption.113}%
\contentsline {figure}{\numberline {7.3}{\ignorespaces Loss in der Adaption auf multilingualen Daten im Modellvergleich.\relax }}{48}{figure.caption.114}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\providecommand \tocbasic@end@toc@file {}\tocbasic@end@toc@file 
