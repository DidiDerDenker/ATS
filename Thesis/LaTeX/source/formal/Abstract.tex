\chapter*{Abstract}
\thispagestyle{empty}

\noindent
Vogel, Daniel: Adaption multilingual vortrainierter Modelle zur automatischen Zusammenfassung von Texten auf die deutsche Sprache, Hochschule für Technik und Wirtschaft Dresden, Fakultät Informatik/ Mathematik, Studiengang Angewandte Informatik, Studienrichtung Data Science, Masterarbeit, 2021.\\[1ex]

\noindent
58 Seiten, 52 Literaturquellen, 2 Anhänge.\\[30ex]

\noindent
In der vorliegenden Arbeit wird die Adaption multilingual vortrainierter Modelle zur automatischen Zusammenfassung von Texten auf die deutsche Sprache erforscht und demonstriert. Hierfür werden entsprechende Grundlagen in Deep Learning und Natural Language Processing dargestellt. Dabei ist insbesondere der kontextbezogene Fortschritt durch Transfer Learning in Verbindung mit Deep Language Representations hervorzuheben. Es schließen sich verschiedene Experimente an, deren Architektur und Datengrundlage zuvor methodisch aufbauend definiert wird.\\

\noindent
Die automatische Zusammenfassung von Texten lässt sich mithilfe eines Sequence-to-Sequence-Transformer-Modells, welches über einen vortrainierten Encoder und Decoder verfügt, SOTA-konform realisieren, insbesondere unter Nutzung von BERT und BART. Die Adaption auf die deutsche Sprache bedarf nicht zwingend einer architektonischen Anpassung, sondern einem Austausch der Textdaten in der entsprechenden Zielsprache, um Zusammenfassungen auf SOTA-Niveau zu generieren. Die Qualität der sprachtechnischen Adaption ist sehr stark von der Qualität der vortrainierten Modelle sowie dem Umfang und der Beschaffenheit der zugrundeliegenden Textdaten abhängig.
