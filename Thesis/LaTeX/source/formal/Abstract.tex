\chapter*{Abstract}
\thispagestyle{empty}

\noindent
Vogel, Daniel: Adaption multilingual vortrainierter Modelle zur automatischen Zusammenfassung von Texten auf die deutsche Sprache, Hochschule für Technik und Wirtschaft Dresden, Fakultät Informatik/ Mathematik, Masterarbeit, 2020.\\[1ex]

\noindent
62 Seiten, 53 Literaturquellen, 4 Anhänge.\\[40ex]

\noindent
Die vorliegende Arbeit beschäftigt sich mit der Adaption multilingual vortrainierter Modelle zur automatischen Zusammenfassung von Texten auf die deutsche Sprache. Hierfür werden zunächst theoretische Grundlagen behandelt, bevor sich Experimente anschließen, welche den SOTA reproduzieren und die entsprechende sprachtechnische Adaption erproben.\\

\noindent
Die automatische Zusammenfassung von Texten lässt sich mithilfe eines Sequence-to-Sequence-Transformer-Modells, welches über einen vortrainierten Encoder und Decoder verfügt, auf SOTA-Niveau realisieren. Die Adaption auf die deutsche Sprache bedarf nicht zwingend einer architektonischen Anpassung, sondern einem Austausch der Textdaten in der entsprechenden Zielsprache, um Zusammenfassungen auf SOTA-Niveau zu generieren. Die Qualität der sprachtechnischen Adaption ist sehr stark von der Qualität der vortrainierten Modelle sowie dem Umfang und der Beschaffenheit der zugrundeliegenden Textdaten abhängig.
