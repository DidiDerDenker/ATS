\addcontentsline{toc}{chapter}{Literaturverzeichnis}
\setcounter{page}{104}

\begin{thebibliography}{104}
\thispagestyle{fancy}

\bibitem[Beltagy et al., 2020]{BEL20}
Beltagy, Iz \& Peters, Matthew \& Cohan, Arman: Longformer as the Long-Document Transformer, Allen Institute for Artificial Intelligence, Seattle, 2020.

\bibitem[Bird et al., 2009]{BIR09}
Bird, Steven \& Klein, Ewan \& Loper, Edward: Natural Language Processing with Python, Verlag O'Reilly, Sebastopol, Vereinigte Staaten, 2009.

\bibitem[Brownlee, 2019]{BRO19}
Brownlee, Jason: A Gentle Introduction to the Bag-of-Words Model, in: \url{https://machinelearningmastery.com/gentle-introduction-bag-words-model/}, Aufruf am 19.05.2021.

\bibitem[Cieliebak, 2019]{CIE19}
Cieliebak, Mark: German Text Summarization Challenge, Swiss Text Analytics Conference, Winterthur, 2019.

\bibitem[Conneau et al., 2020]{CON20}
Conneau, Alexis \& Khandelwal, Kartikay \& Goyal, Naman \& Chaudhary, Vishrav \& Wenzek, Guillaume \& Guzman, Francisco \& Grave, Edouard \& Ott, Myle \& Zettlemoyer, Luke \& Stoyanov, Veselin: Unsupervised Cross-Lingual Representation Learning at Scale, Facebook AI, 2020.

\bibitem[CS231N, O. J.]{CSNOJ}
Convolutional Neural Networks for Visual Recognition: Nesterov Momentum, in: \url{https://cs231n.github.io/neural-networks-3/}, Aufruf am 16.04.2021.

\bibitem[Culurciello, 2018]{CUR18}
Culurciello, Eugenio: The Fall of RNN/ LSTM, in: \url{https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0}, Aufruf am 23.04.2021.

\bibitem[Devlin et al., 2019]{DEV19}
Devlin, Jacob \& Chang, Ming-Wei \& Lee, Kenton \& Toutanova, Kristina: Pre-training of Deep Bidirectional Transformers for
Language Understanding, Google AI Language, 2019.

\bibitem[Ding et al., 2020]{DIN20}
Ding, Ming \& Zhou, Chang \& Yang, Hongxia \& Tang, Jie: Applying BERT to Long Texts, in: \url{https://proceedings.neurips.cc/paper/2020/file/96671501524948bc3937b4b30d0e57b9-Paper.pdf}, Aufruf am 26.05.2021.

\bibitem[Edpresso, O. J.]{EDPOJ}
Edpresso: Overfitting and Underfitting, in: \url{https://www.educative.io/edpresso/overfitting-and-underfitting}, Aufruf am 09.04.2021.

\bibitem[Gambhir et al., 2016]{GAM16}
Gambhir, Mahak \& Gupta, Vishal: Recent Automatic Text Summarization Techniques, University of Panjab in Chandigargh, 2016.

\bibitem[Goncalves, 2020]{GON20}
Goncalves, Luis: Automatic Text Summarization with Machine Learning, in: \url{https://medium.com/luisfredgs/automatic-text-summarization-with-machine-learning-an-overview-68ded5717a25}, Aufruf am 16.03.2021.

\bibitem[Goodfellow et al., 2016]{GOO16}
Goodfellow, Ian \& Bengio, Yoshua \& Courville, Aaron: Deep Learning, in: \url{https://www.deeplearningbook.org/}, Aufruf am 03.07.2021.

\bibitem[HuggingFace, 2021]{HUG21}
HuggingFace: The AI community building the future, in: \url{https://huggingface.co/}, Aufruf am 14.06.2021.

\bibitem[Huilgol, 2020]{HUI20}
Huilgol, Purva: Quick Introduction to Bag-of-Words (BoW) and TF-IDF for Creating Features from Text, in: \url{https://www.analyticsvidhya.com/blog/2020/02/quick-introduction-bag-of-words-bow-tf-idf/}, Aufruf am 19.05.2021.

\bibitem[Irene, 2018]{IRE18}
Irene: ELMo in Practice, in: \url{https://ireneli.eu/2018/12/17/elmo-in-practice/}, Aufruf am 26.05.2021.

\bibitem[Karani, 2018]{KAR18}
Karani, Dhruvil: Introduction to Word Embedding and Word2Vec, in: \url{https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa}, Aufruf am 19.05.2021.

\bibitem[Karim, 2019]{KAR19}
Karim, Raimi: Self-Attention, in: \url{https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a}, Aufruf am 26.05.2021.

\bibitem[Khanna, 2019]{KHA19}
Khanna, Sachin: Machine Learning vs. Deep Learning, Department of Computer Science Engineering in India, 2019.

\bibitem[Kiani, 2017]{KIA17}
Kiani, Farzad: Automatic Text Summarization, University of Arel in Istanbul, 2017.

\bibitem[Kingma et al., 2017]{KIN17}
Kingma, Diederik \& Ba, Jimmy: Adam - A Method for Stochastic Optimization, University of Amsterdam and Toronto, 2017.

\bibitem[Lemberger, 2020]{LEM20}
Lemberger, Pirmin: Deep Learning Models for Automatic Summarization, in: \url{https://towardsdatascience.com/deep-learning-models-for-automatic-summarization-4c2b89f2a9ea}, Aufruf am 26.05.2021.

\bibitem[Lewis et al., 2019]{LEW19}
Lewis, Mike \& Liu, Yinhan \& Goyal, Naman \& Ghazvininejad, Marjan \& Mohamed, Abdelrahman \& Levy, Omer \& Stoyanov, Ves \& Zettlemoyer, Luke: BART as Denoising Sequence-to-Sequence Pre-Training for Natural
Language Generation, Translation and Comprehension, Facebook AI, 2019.

\bibitem[Lin, 2004]{LIN04}
Lin, Chin-Yew: ROUGE as a Package for Automatic Evaluation of Summaries, Information Sciences Institute, Southern California, 2004.

\bibitem[Manning et al., 2008]{MAN08}
Manning, Christopher \& Raghavan, Prabhakar \& Schütze, Heinrich: Introduction to Information Retrieval, Cambridge University Press, 2008.

\bibitem[McCullum, 2020]{MCC20}
McCullum, Nick: Deep Learning Neural Networks Explained, in: \url{https://www.freecodecamp.org/news/deep-learning-neural-networks-explained-in-plain-english/}, Aufruf am 09.04.2021.

\bibitem[Moberg, 2020]{MOB20}
Moberg, John: A Deep Dive into Multilingual NLP Models, in: \url{https://peltarion.com/blog/data-science/a-deep-dive-into-multilingual-nlp-models}, Aufruf am 19.05.2021.

\bibitem[Nallapati et al., 2016]{NAL16}
Nallapati, Ramesh \& Zhou, Bowen \& Dos Santos, Cicero \& Gulcehre, Caglar \& Xiang, Bing: Abstractive Text Summarization using Sequence-to-Sequence RNNs, Conference on Computational Natural Language Learning, 2016.

\bibitem[Nitsche, 2019]{NIT19}
Nitsche, Matthias: Towards German Abstractive Text Summarization using Deep Learning, HAW Hamburg, 2019.

\bibitem[NLTK, 2020]{NLT20}
NLTK: Stem-Package, in: \url{https://www.nltk.org/api/nltk.stem.html}, Aufruf am 19.05.2021.

\bibitem[NVIDIA, 2021]{NVI21}
NVIDIA: CUDA Toolkit, in: \url{https://developer.nvidia.com/cuda-toolkit}, Aufruf am 14.06.2021.

\bibitem[Papineni et al., 2002]{PAP02}
Papineni, Kishore \& Roukos, Salim \& Ward, Todd \& Zhu, Wei-Jing: BLEU as a Method for Automatic Evaluation of Machine Translation, Association for Computational Linguistics, Philadelphia, 2002.

\bibitem[Paulus et al., 2017]{PAU17}
Paulus, Romain \& Xiong, Caiming \& Socher, Richard: A Deep Reinforced Model for Abstractive Summarization, in: \url{https://arxiv.org/pdf/1705.04304v3.pdf}, Aufruf am 16.03.2021.

\bibitem[Penninglon et al., 2014]{PEN14}
Penninglon, Jeffrey \& Socher, Richard \& Manning, Christopher: Global Vectors for Word Representation, Stanford University, 2014.

\bibitem[Peters et al., 2018]{PET18}
Peters, Matthew \& Neumann, Mark \& Iyyer, Mohit \& Gardner, Matt \& Clark, Christopher \& Lee, Kenton \& Zettlemoyer, Luke: Deep Contextualized Word Representations, Allen Institute for Artificial Intelligence, Washington, 2018.

\bibitem[Radford et al., 2019]{RAD19}
Radford, Alec \& Wu, Jeff \& Child, Rewon \& Luan, David \& Amodei, Dario \& Sutskever, Ilya: Language Models are Unsupervised Multitask Learners, in: \url{https://openai.com/blog/better-language-models/}, Aufruf am 19.05.2021.

\bibitem[Raffel et al., 2020]{RAF20}
Raffel, Colin \& Shazeer, Noam \& Roberts, Adam \& Lee, Katherine \& Narang, Sharan \& Matena, Michael \& Zhou, Yanqi \& Li, Wei \& Lio, Peter: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, Google Research, 2020.

\bibitem[Raschka et al., 2019]{RAS19}
Raschka, Sebastian \& Mirjalili, Vahid: Machine Learning and Deep Learning with Python, Verlag Packt, Birmingham, Vereinigtes Königreich, 2019.

\bibitem[Rothe et al., 2020]{ROT20}
Rothe, Sascha \& Narayan, Shashi \& Severyn, Aliaksei: Leveraging Pre-Trained Checkpoints for Sequence Generation Tasks, Google Research, 2020.

\bibitem[Scialom et al., 2020]{SCI20}
Scialom, Thomas \& Dray, Paul-Alexis \& Lamprier, Sylvain \& Piwowarski, Benjamin \& Staiano, Jacopo: MLSUM as the Multilingual Summarization Corpus, Sorbonne Université, Paris, 2020.

\bibitem[SpaCy, 2021]{SPA21}
SpaCy: German Models, in: \url{https://spacy.io/models/de}, Aufruf am 19.05.2021.

\bibitem[Sprachenfabrik, 2016]{SPR16}
Sprachenfabrik: Eigenarten der deutschen Sprache, in: \url{https://sprachenfabrik.de/de/2016/01/18/7-eigenarten-der-deutschen-sprache/}, Aufruf am 07.07.2021.

\bibitem[TensorFlow, 2021]{TEN21}
TensorFlow: Datasets - CNN-DailyMail, in: \url{https://www.tensorflow.org/datasets/catalog/cnn_dailymail}, Aufruf am 23.03.2021.

\bibitem[Thaker, 2019]{THA19}
Thaker, Madhav: Comparing Text Summarization Techniques, in: \url{https://towardsdatascience.com/comparing-text-summarization-techniques-d1e2e465584e}, Aufruf am 23.03.2021.

\bibitem[Vaswani et al., 2017]{VAS17}
Vaswani, Ashish \& Shazeer, Noam \& Parmar, Niki \& Uszkoreit, Jakob \& Jones, Llion \& Gomez, Aidan \& Kaiser, Lukasz \& Polosukhin, Illia: Attention Is All You Need, Google Research, 2017.

\bibitem[Von Platen, 2020]{VON20}
Von Platen, Patrick: Leveraging Pre-trained Language Model Checkpoints for Encoder-Decoder Models, in: \url{https://huggingface.co/blog/warm-starting-encoder-decoder}, Aufruf am 16.03.2021.

\bibitem[Wissler et al., 2014]{WIS14}
Wissler, Lars \& Almashraee, Mohammed \& Monett, Dagmar \& Pasche, Adrian: The Gold Standard in Corpus Annotation, Free University, Berlin, 2014.

\bibitem[Yang et al., 2019]{YAN19}
Yang, Liu \& Lapata, Mirella: Text Summarization with Pretrained Encoders, Institute for Language, Cognition and Computation in Edinburgh, 2019.

\bibitem[Yang et al., 2020]{YAN20}
Yang, Li \& Shami, Abdallah: On Hyperparameter Optimization of Machine Learning Algorithms, University of Western Ontario, 2020.

\bibitem[Zaheer et al., 2021]{ZAH21}
Zaheer, Manzil \& Guruganesh, Guru \& Dubey, Avinava \& Ainslie, Joshua \& Alberti, Chris \& Ontanon, Santiago \& Pham, Philip \& Ravula, Anirudh \& Wang, Qifan \& Yang, Li \& Ahmed, Amr: Bid Bird as a Transformer for Longer Sequences, Google Research, 2020.

\bibitem[Zhang et al., 2020]{ZHA20}
Zhang, Aston \& Lipton, Zachary \& Li, Mu \& Smola, Alexander: Dive into Deep Learning, in: \url{https://d2l.ai/}, Aufruf am 09.04.2021.

\bibitem[ZIH, 2021]{ZIH21}
Zentrum für Informationsdienste und Hochleitungsrechnen der TU Dresden: Hochleitungsrechnen, in: \url{https://tu-dresden.de/zih/hochleistungsrechnen}, Aufruf am 14.06.2021.

\end{thebibliography}
