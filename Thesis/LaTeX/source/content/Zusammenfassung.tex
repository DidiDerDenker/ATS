\chapter{Zusammenfassung}
\thispagestyle{fancy}
\label{chap:Zusammenfassung}

\noindent
In dieser Arbeit wurde die Adaption multilingual vortrainierter Modelle zur automatischen Zusammenfassung von Texten auf die deutsche Sprache erforscht und demonstriert. Hierfür wurden entsprechende Grundlagen in \ac{DL} und \ac{NLP} offengelegt. Dabei ist insbesondere der kontextbezogene Fortschritt durch \ac{TL} in Assoziation mit den eingeführten \ac{DLR} hervorzuheben. Um das Ziel dieser Arbeit zu erreichen und die Forschungsfragen hinreichend zu beantworten, schlossen sich verschiedene Experimente an, deren Architektur und Datengrundlage zuvor methodisch aufbauend definiert wurde.\\

\noindent
Das Ziel dieser Arbeit, multilingual vortrainierte Modelle mittels \ac{TL} auf die deutsche Sprache zu adaptieren, konnte mithilfe von \ac{TF2TF} unter Nutzung von \ac{BERT} und \ac{BART} erreicht werden. Die entsprechenden Architekturen erreichen den \ac{SOTA} der \ac{ATS} in Englisch sowie in Deutsch. Die analysierten qualitativen Schwächen sind jeweils mit existierenden \ac{SOTA}-Modellen vergleichbar und daher akzeptabel. Hierbei ist außerdem anzumerken, dass mit HuggingFace selbst eines der größten Unternehmen der Branche, welches theoretisch über entsprechende Ressourcen verfügt, keine optimalen Ergebnisse erzielt. Es folgt die Beantwortung der einleitend formulierten Forschungsfragen.
\newpage

\noindent
\textbf{Wie lassen sich Texte automatisiert zusammenfassen?}\\
\noindent
Hierfür bedarf es einer Architektur, welche die Grundlagen des \ac{NLP} vereint, entsprechend vortrainierte \ac{DLR} integriert und ein weitergehendes Fine-Tuning auf möglichst vielen paarweisen Textdaten ermöglicht, um die Herausforderungen des \ac{NLU} und der {NLG} zu bewältigen. Dies wird beispielsweise von einem Sequence-to-Sequence-Transformer-Modell, welches über einen Encoder und einen Decoder verfügt, erfüllt.\\

\noindent
\textbf{Wie können existierende Modelle auf eine andere Sprache adaptiert werden?}\\
\noindent
Hierfür sind nicht zwingend architektonische Anpassungen notwendig, sondern vielmehr ein Austausch der Textdaten in der entsprechenden Zielsprache. Die oben beschriebene Architektur ist folglich in der Lage, sprachabhängige Strukturen dynamisch zu erlernen. Dabei wird die Qualität der Adaption entscheidend von den vortrainierten Modellen sowie den zugeführten Textdaten beeinflusst. Hier wird im folgenden Kapitel hinreichend angeknüpft.\\

\noindent
\textbf{Wie qualitativ und skalierbar ist die Lösung?}\\
\noindent
Während die untersuchten Architekturen wertmäßig den \ac{SOTA} erreichen, konnten qualitativ verschiedene Eigenarten identifiziert werden. Dies ist jedoch meist nicht den Architekturen selbst anzurechnen, sondern den zugeführten Textdaten, da sich die Architekturen in anderen Sprachen mit entsprechend umfangreichen und qualitativen Textdaten bereits bewährt haben. Daher ist gleichermaßen nicht davon auszugehen, dass die Architekturen größere Probleme mit der deutschen Sprache haben. Wurde ein Modell erst einmal hinreichend trainiert, kann es veröffentlicht, in Anwendungen implementiert und somit entsprechend skaliert werden. Hierbei ist das Echtzeitverhalten jedoch hinreichend zu analysieren, insofern dies im Anwendungsfall relevant ist.
