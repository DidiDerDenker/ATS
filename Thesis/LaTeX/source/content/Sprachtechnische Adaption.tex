\chapter{Sprachtechnische Adaption}
\thispagestyle{fancy}
\label{chap:Sprachtechnische Adaption}

\noindent
Unter Kenntnis der Architektur des abstraktiven Ansatzes und der Baseline des englischsprachigen Modells wird nun ergründet, wie eine Adaption auf die deutsche Sprache erfolgen kann. Dies wird mithilfe verschiedener Experimente erprobt, welche sich weiterhin auf die bekannten Metriken stützen.


\section{Architektur}
\noindent
Die bislang genutzte Architektur, welche bekanntermaßen Transformer integriert, kann auf Grundlage umfangreicher verschiedensprachiger ungelabelter Textdaten im Sinne des \ac{DL} und \ac{TL} multilingual vortrainiert werden, ohne architektonische Anpassungen vornehmen zu müssen. Hierbei werden sprachübergreifende verborgene Strukturen erlernt, um anschließend monolingual davon zu profitieren. Zudem wird dem Problem entgegengewirkt, dass sprachintern zu wenig Textdaten zur Verfügung stehen \cite{MOB20}. Modelle, welche organisationsextern bereits multilingual vortrainiert und bereitgestellt wurden, sind beispielsweise die multilinguale Version von \ac{BERT} und die \ac{XLM-R}. Hinsichtlich einer anschließenden deutschsprachigen Nutzung in der \ac{ATS} ist weiterhin ein entsprechendes sprachbezogenes Training erforderlich. Dies wird im Verlauf der Experimente hinreichend abgehandelt.


\section{Experimente}
\noindent
In einem initialen Experiment wird die multilinguale Version von \ac{BERT} mit einem Mix aus allen verfügbaren deutschsprachigen Daten trainiert. Hierbei entstehen die folgenden \ac{ROUGE}-Scores: R-Recall: 24.56, R-Precision: 15.05, R-Measure: 17.25. Dies überliegt dem \ac{SOTA} zwar deutlich, ist allerdings auf die Struktur der Trainingsdaten zurückzuführen, da mehr als die Hälfte aller Daten aus Wikipedia-Artikeln stammen.
\newpage

\noindent
Dennoch sei nachfolgend die Zusammenfassung des in Anhang B einzusehenden deutschsprachigen Textes gezeigt. Dieser gleicht strukturell sowie inhaltlich dem bereits beschriebenen Text in Anhang A. In der Folge schließen sich verschiedene jeweils aus dem vorherigen Schritt abgeleitete Experimente an.\\

\noindent\fbox{%
\parbox{\textwidth}{%
In New York ist ein Flugzeug aus dem World Trade Center auf dem Nordturm des World Trade Centers gebrannt. In den USA ist es zu einem Unfall gekommen. Nun ist das Land wieder in Schock, wo es sich um einen Brand ereignete. Es war der erste Unfall, der sich in den USA ereignet hat.
}%
}
\newline

\noindent
Die inhaltliche Schwäche ist für den menschlichen Leser auch ohne Kenntnis über den Originaltext unschwer erkennbar. Daher wird nun die multilinguale Version von \ac{BERT} durch eine eigens für die deutsche Sprache vortrainierte Version ausgetauscht. Hierbei handelt es sich um den sogenannten \ac{GBERT}, welcher von Deepset AI, einem globalen Anbieter von Open-Source-Lösungen für \ac{NLP}, vortrainiert und bereitgestellt wird. An der deutschsprachigen Datengrundlage wird zunächst nichts verändert. Hierbei entstehen die folgenden \ac{ROUGE}-Scores: R-Recall: 28.51, R-Precision: 16.91, R-Measure: 19.59. Nachfolgend ist erneut eine Zusammenfassung des Textes aus Anhang B zu sehen.\\

\noindent\fbox{%
\parbox{\textwidth}{%
...
}%
}\\



Letzterer scheint tatsächlich von den verborgenen Strukturen anderer Sprachen zu profitieren.\\

% TODO: Andere neuere deutsche Version ausprobieren, ggf. doch damit weiter machen und oben rechtfertigen, Relevanz des Vortrainings verdeutlichen, untere Absätze aktualisieren

\noindent
Trotz der monolingualen Fortschritte wird \ac{GBERT} zunächst durch \ac{XLM-R} und anschließend durch \ac{BART} ersetzt, um sich weiterhin geeigneten Ergebnissen zu nähern. Der Tokenizer wird ebenfalls entsprechend ausgetauscht. Es entstehen die folgenden \ac{ROUGE}-Scores: R-Recall: 23.02, R-Precision: 13.95, R-Measure: 16.09. Obgleich \ac{XLM-R} bewiesenermaßen verschiedene \ac{NLP}-Aufgaben begünstigt, kann sie in der vorliegenden Konfiguration keine Verbesserung der \ac{ATS} erzielen. Dies basiert nicht nur auf den verringerten \ac{ROUGE}-Scores, sondern auch auf der nachfolgenden exemplarischen Zusammenfassung von Anhang B, welche inhaltlich subjektiv als mangelhaft eingestuft werden kann.\\

\noindent\fbox{%
\parbox{\textwidth}{%
Der 11. Flugtag der Weltturm-Weltmeisterschaft 2001 fand am 11. September 2001 in New York City statt und war das erste Mal in der Geschichte des World Trade Centers. Das National September 11 Memorial and Museum in Manhattan ist ein historischer Gedenkpavillon in Manhattan. Es befindet sich in der Nähe des World Trade Centers in Manhattan.
}%
}\\

% TODO: BART trainieren und kritisieren, evtl. ist GBERT trotzdem besser

\noindent
In der Konsequenz wird \ac{GBERT} für die weiteren Experimente genutzt. Hierbei werden insbesondere die Anteile der jeweiligen Korpora an den Trainings- und Testdaten variiert, um vornehmlich dem Lernen der Textstruktur entgegenzuwirken. Die Konfigurationen sowie entsprechende Ergebnisse sind der Tabelle ? zu entnehmen.\\

% TODO: Tabelle ? nach erfolgten Trainingsschritten erstellen, oben referenzieren und Erkenntnisse beschreiben/ herausstellen, weitere Schritte begründet anschließen, bspw. Data Augmentation, Übersetzung englischsprachiger Korpora, Experimente im Excel kompaktieren, bspw. letztes Tabellenblatt

\noindent
- Training auf Wiki, Evaluation auf News
  -> Sehr schlecht, strukturerlernend
- Training auf News mit ein bisschen Wiki, Evaluation mit New
   -> Positiver Effekt, aber zu wenig Daten
- Training auf News und 50 Prozent von Wiki, Evaluation mit News
  -> ?
- Training auf übersetztem CNN/ Dailymail, Evaluation mit News
  -> ?

% TODO: Sliding-Window-Approach bei zu großen Texten beschreiben und entwickeln, beim Laden der Texte, aber als separate Methode, die auf einzelne Texte anwendbar ist, trotzdem mit Map als Batch-Verarbeitung, über Bool in der Config beim Training auswählbar machen, im Beispiel als Methode einbinden

% TODO: Kapitel besser strukturieren (format-technisch, inhaltlich), German Abstractive Summarization lesen und einarbeiten, Beschreibung der Datengrundlage nochmal überprüfen, Lorem ipsum und TODO's in beiden Kapiteln herausarbeiten, Hyperparameteroptimierung auf 10-20 Prozent der Daten vornehmen, Verdichtung der Zusammenfassung verdeutlichen, d.h. Token-Reduktion, zudem die prozentuale Kompressionsrate angeben (75% von 512 auf 128 Token), d.h. mit technischen Anpassungen können auch 5 DIN A4-Seiten um x Prozent verdichtet werden (Wie lang ist der Eingangstext? Wie lang ist der Ausgangstext? Wie geht das Modell mit längeren Texten um?)

% TODO: \cite{YAN19} S. 4 rechts, Herausforderung: Encoder overfitted, Decoder underfitted oder andersherum, wird durch HuggingFace-Framework vorgebeugt, \cite{YAN19} S. 5 oben für Evaluation, Vergleichstabelle der Experimente einbinden und beschreiben, typisches Diagramm zur Visualisierung des Trainingsprozesses anfügen, Verhalten des Modells interpretieren und Anpassungen ableiten, bspw. Exploitation wegen der Struktur der Artikel nochmal aufgreifen, ggf. erst bei der sprachtechnischen Adaption, erwähnen, dass dies als Experiment genügt, sprachtechnische Anpassungen dann erst im nächsten Kapitel, Referenzzusammenfassungen mit ROUGE und BLEU bewerten, um Vergleichswerte nennen zu können, Texte manuell zusammenfassen, um ebenfalls einen Vergleichswert von ROUGE und BLEU zu haben\\

% TODO: Dateien vom Taurus herunterladen und synchronisieren
