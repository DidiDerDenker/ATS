\chapter{Abstraktiver Ansatz}
\thispagestyle{fancy}
\label{chap:Abstraktiver Ansatz}

Notizen:
\begin{itemize}
	\item Abgrenzung zum extraktiven Ansatz beschreiben
	\item Vorteile gegenüber referenzierten Modellen herausstellen
	\item Generierung neuer Sätze sowohl mit vorkommenden als auch mit nicht-vorkommenden Wörtern (vgl. Paper: „Automatic Text Summarization with Machine Learning“)
	\item Verschiedene Ansätze \url{https://medium.com/analytics-vidhya/deep-reinforcement-learning-deeprl-for-abstractive-text-summarization-made-easy-tutorial-9-c6914999c76c}, evtl. im Forschungsstand erwähnen
	\item Siehe Abstract im Exposé
\end{itemize}


\section{Architektur}
Notizen:
\begin{itemize}
	\item Netzwerk des abstraktiven Ansatzes als Pipeline skizzieren
	\item Seq2Seq \url{https://github.com/yaserkl/RLSeq2Seq#dataset}
	\item Deep Reinforcement Learning (DeepRL) for Abstractive Text Summarization \url{https://medium.com/analytics-vidhya/deep-reinforcement-learning-deeprl-for-abstractive-text-summarization-made-easy-tutorial-9-c6914999c76c}
\end{itemize}


\section{Konfiguration}
Notizen:


\section{Training}
Notizen:
\begin{itemize}
	\item Training verschiedener Modelle
\end{itemize}


\section{Evaluation}
Notizen:
\begin{itemize}
	\item Kompressionsrate messen
	\item Qualität der Zusammenfassung messen (BLEU \url{https://en.wikipedia.org/wiki/BLEU}, ROUGE \url{https://en.wikipedia.org/wiki/ROUGE_(metric)}, evtl. Funktionen fusionieren)
	\item Evaluation verschiedener Modelle mit geeigneter Vergleichstabelle
	\item Vergleich mit SOTA-Modellen
	\item Praktische Nutzung durch Implementation eines vortrainierten Modells in ein Skript oder eine Software
	\item Es muss eine Metrik existieren, mit der man die Genauigkeit bzw. Qualität der Zusammenfassung messen kann, d.h. man möchte die Texte nicht mit menschlich generierten Zusammenfassungen vergleichen, sondern automatisiert lernen, ggf. sollte man auch Grammatik und Inhalt separat prüfen
	\item For a given document there is no summary which is objectively the best. As a general rule, many of them that would be judged equally good by a human. It is hard to define precisely what a good summary is and what score we should use for its evaluation. Good training data has long been scarce and expensive to collect. Human evaluation of a summary is subjective and involves judgments like style, coherence, completeness and readability. Unfortunately no score is currently known which is both easy to compute and faithful to human judgment. The ROUGE score [6] is the best we have but it has obvious shortcomings as we shall see. ROUGE simply counts the number of words, or n-grams, that are common to the summary produced by a machine and a reference summary written by a human. \url{https://towardsdatascience.com/deep-learning-models-for-automatic-summarization-4c2b89f2a9ea}
	\item Bei der Anwendung einer Architektur, in der das Modell durch Reinforcement Learning trainiert wird, braucht man keine massenhaft menschlich generierten Referenztexte, sondern eine wohlbedachte Kostenfunktion, der ein entsprechender Aufwand entgegen gebracht werden muss, d.h. die Herausforderung liegt beim RL eher darin, eine Umwelt und eine geeignete Funktion zum Belohnen und Bestrafen zu konstruieren, hier sind bspw. auch Evaluationsmetriken notwendig
	\item Rouge-Score in Python: \url{https://pypi.org/project/rouge-score/}
	\item Typisches Diagramm zur Visualisierung des Trainingsprozesses anfügen
\end{itemize}
