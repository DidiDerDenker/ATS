\chapter{Deep Learning}
\thispagestyle{fancy}
\label{chap:Deep Learning}

Notizen:
\begin{itemize}
	\item Deep Learning definieren
	\item Machine Learning erwähnen
	\item Siehe Abstract im Exposé
	\item ATS on GitHub: \url{https://github.com/mathsyouth/awesome-text-summarization#corpus}
\end{itemize}


\section{Neuronale Netze}
Notizen:
\begin{itemize}
	\item Neuronale Netze definieren
	\item Historie beschreiben
	\item Funktionsweise und ausgewählte Komponenten beschreiben
\end{itemize}


\section{Architekturen}
Notizen:
\begin{itemize}
	\item Existenz und Notwendigkeit verschiedener Architekturen ankündigen, ggf. in spätere Kapitel verlegen, bspw. zum abstraktiven Ansatz
	\item Später benötigte Architekturen hier beschreiben
	\item Diversität der existierenden Architekturen (wie im Forschungsstand bereits erwähnt) hervorheben
	\item "Reinforcement Learning comes to the rescue" aus \url{https://towardsdatascience.com/deep-learning-models-for-automatic-summarization-4c2b89f2a9ea} einbinden
	\item Encoder/ Decoder, Self-Attention, Seq to Seq, Transformer Model (Recherche + Vergleich)
	\item Transformer, bestehend aus Seq-to-Seq-Model mit Encoder-/ Decoder-Architektur, gut erklärt: \url{https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04}, wissenschaftliche Paper hierzu: \url{https://arxiv.org/abs/1706.03762}, \url{https://wiki.pathmind.com/}, \url{https://nlp.stanford.edu/pubs/emnlp15_attn.pdf}, Struktur ggf. überarbeiten, d.h. langsam an Seq to Seq, Encoder, Decoder heranführen
	\item PyCharm-Rebuild: Seq-to-Seq with Local Attention-: \url{https://github.com/JRC1995/Abstractive-Summarization}, \url{https://nlp.stanford.edu/projects/glove/}, \url{https://nlp.stanford.edu/pubs/emnlp15_attn.pdf}, \url{https://arxiv.org/abs/1409.3215}, \url{https://arxiv.org/abs/1409.0473}
	\item PyCharm-Rebuild: Bert-Encoder with Transformer-Decoder: \url{https://github.com/santhoshkolloju/Abstractive-Summarization-With-Transfer-Learning}
	\item PyCharm-Rebuild: RL-Seq-to-Seq: \url{https://github.com/yaserkl/RLSeq2Seq}, \url{https://arxiv.org/abs/1805.09461}
\end{itemize}


\subsection{MLP}
Notizen:


\subsection{RNN, LSTM}
Notizen:


\subsection{Seq2Seq, Encoder-Decoder}
Notizen:


\subsection{Attention, Multi-Head-Attention, Self-Attention}
Notizen:


\section{Hyperparameter}
Notizen:
\begin{itemize}
	\item Hyperparameter vorstellen
	\item Notwendigkeit und Einfluss von Hyperparametern beschreiben
	\item Batch-Size, e.g. Mini-Batch vs. Stochastic Batch: \url{https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network}
\end{itemize}


\section{Transfer Learning}
Notizen:
\begin{itemize}
	\item Transfer Learning mit BERT hier sinnvoll, sodass das Modell die Sprache nicht in einer bestimmten Domain oder mit zu wenigen Texten neu erlernen muss
	\item BERT zunächst in Englisch nutzen, weil SOTA, ggf. Grafik aus Oli's VL integrieren, irgendwie in Abstractive Summarization Pipelines integrieren
	\item BERT ist auch multilingual, d.h. englisches Modell mit deutschem Fine-Tuning vermutlich sogar brauchbar
	\item Modell beschreiben, d.h. Datensätze und Parametrisierung, SOTA für verschiedene NLP-Tasks, von Kapazitäten profitieren, hat viel Kontextwissen, Fine-Tuning für eigenes Problem, d.h. domainspezifisch o.ä.
	\item Am besten direkt ein vortrainiertes Transformer-Modell nutzen (extra für Summarization-Tasks), BERT und RL bspw. in der Pipeline integrieren, Ziel wäre dann: Verbesserung im Score erzielen
	\item BERT vielleicht durch andere (teils bessere und neuere) Transformer ersetzen? Transformer in NLP recherchieren, LSTM als veraltet bezeichnen
\end{itemize}
