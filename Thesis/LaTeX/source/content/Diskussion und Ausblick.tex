\chapter{Diskussion und Ausblick}
\thispagestyle{fancy}
\label{chap:Diskussion und Ausblick}

\noindent
Die bislang schlichtweg evaluierten und zusammengefassten Ergebnisse werden nun einer weitergehenden Diskussion mit anschließendem Ausblick unterzogen. Dabei werden die erarbeiteten Ergebnisse kontextuell eingeordnet, mitunter kritisiert und weitere Forschungsaufgaben abgeleitet.\\

\noindent
...


- NLU gut, NLG herausfordernd
- Hyperparameteroptimierung bei Transformern schwierig, nur bei TF2TF als Einheit gewinnbringend
- Umgang mit kurzen und langen Texten analysieren
- Training vor Overfitting stoppen, ggf. geeignete Metriken dafür finden, ggf. auch, um Generalisierung zu messen, anscheinend großes Problem
- Zukünftig mehr Aufmerksamkeit auf die Kollektion von Daten notwendig, Modell lernt datenentsprechend (eher Proof of Concept, d.h. Fortschritte erkennbar, Funktionalität der Modelle bewiesen, aber unbedingt mehr Daten), d.h. mit qualitativen Daten sind ggf. nicht mal architektonische Anpassungen nötig, sondern nur ein erneutes Training auf einer veränderten Datengrundlage, ggf. auch Data Augmentation oder Übersetzungen denkbar, vermuteter Grund: Zu wenig Daten und daher überparametrisiert, Daten ggf. kritisieren, Fortschritte in Deutsch erprobt, in Englisch validiert, DLR und spätere Architekturen sehr datenabhängig, gleiche Wörter in verschiedenen Domänen etwa in unterschiedlichen Kontexten, Probleme bei OOV-Wörtern
- Exploitation wegen der Struktur der Artikel nochmal aufgreifen, Optimierung auf bewährtem Modell mit 10-20 Prozent
- Adaption auf Sprache, Domäne, Textart (Abstractive Dialogue Summarization with Sentence-Gated Modeling Optimized by Dialogue Acts/ Using a KG-Copy Network for Non-Goal Oriented Dialogues/ Automatic Dialogue Summary Generation of Customer Service/ Global Summarization of Medical Dialogue by Exploiting Local Structures, Fine-Tuning in Zieldomäne immer notwendig, d.h. wenn bekannt ist, dass das Modell für medizinische Texte angewandt werden soll, sollte man das Fine-Tuning mit entsprechenden Daten ausführen, ggf. erneut Hyperparameter optimieren, Verdichtung von Protokollen einer Videosprechstunde, Gefahr: Initial bereits hohe Informationsdichte bei medizinischen Texten wie Diktaten, d.h. "Was fällt raus?")
- Pareto-Prinzip für Maschine wie für Menschen auch, daher Empfehlung > 1 Mio. Daten, weil hinten raus viel Detailarbeit passiert
- Sliding Window Approach (teilen, zusammenfassen, konkatenieren), teilweise bereits implementiert/ mitgeliefert
- Limitations von NLP: \cite[S.~30-31]{BIR09}
- Adaptive Learning
- BART von einem der größten Unternehmen der Branchen vortrainiert, immer noch nicht perfekt, Fortschritte erkennbar, auch multilinguale Adaptivität, dennoch selbst trainierter BERT bis auf wenige strukturelle Schwächen fast genauso gut
- Kompressionsrate in Form von Token-Reduktion angeben, technische Anpassungen andeuten (z.B. Aussagen wie "75 Prozent Kompression, d.h. 512 Token auf 128 Token reduzierbar, aber Art der ATS und Höhe der Kompression im Training steuerbar, d.h. theoretisch auch DIN A4-Seiten reduzierbar")

% Notizen:
- \url{https://reposit.haw-hamburg.de/bitstream/20.500.12738/9137/1/mnitsche_master_state_20190829.pdf}
- \url{https://www.inf.uni-hamburg.de/en/inst/ab/lt/teaching/theses/completed-theses/2020-ma-johner.pdf}
- \cite{YAN19} S. 4 rechts, Herausforderung: Encoder overfitted, Decoder underfitted oder andersherum, wird durch HuggingFace-Framework vorgebeugt
