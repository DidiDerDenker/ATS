\chapter{Diskussion und Ausblick}
\thispagestyle{fancy}
\label{chap:Diskussion und Ausblick}

\noindent
Die bislang schlichtweg evaluierten, analysierten und zusammengefassten Ergebnisse werden nun einer weitergehenden Diskussion mit anschließendem Ausblick unterzogen. Dabei werden die erarbeiteten Ergebnisse kontextuell eingeordnet, mitunter kritisiert und weitere Forschungsaufgaben abgeleitet.\\

\noindent
Zunächst ist zu erkennen, dass die trainierten Modelle datenentsprechend lernen. Die Lernfortschritte in Englisch und auch nach erfolgter Adaption in Deutsch bestätigen die Eignung der \ac{TF2TF} für die \ac{ATS}, insbesondere unter Nutzung von \ac{BERT} und \ac{BART}. Dabei können die Herausforderungen des \ac{NLU} und der \ac{NLG} bewältigt werden. Um die Qualität der Adaption vor allem im deutschsprachigen Raum hinreichend zu verbessern, bedarf es unbedingt einer überarbeiteten Datengrundlage. Hat diese keinen angemessenen Umfang, so sind die trainierten Modelle gemäß Overfitting überparametrisiert. Dies führt mitunter dazu, dass Wörter, welche je nach Domäne in unterschiedlichen Kontexten vorkommen, falsch verstanden werden. Außerdem entstehen Probleme bei Wörtern, die den Wortschatz der trainierten Modelle übersteigen. Damit eben jene Modelle besser generalisieren, ist also zunächst der Umfang der Datengrundlage deutlich zu erweitern. Dabei sind stets die bekannten Anforderungen zu erfüllen. Aufgrund des qualitativen Einflusses sind hierbei unbedingt die Eigenarten der verwendeten Textdaten zu analysieren und entsprechend vorzuverarbeiten. Hier bietet es sich an, Textdaten der Zieldomäne zu nutzen, um die Qualität der \ac{ATS} im Kontext des jeweiligen Anwendungsfalls zu erhöhen. Eine allgemeinsprachliche Nutzung bedarf entsprechend noch umfangreicherer Textdaten. Den Umfang der Datengrundlage gilt es also stets zu maximieren, während die Eigenarten der Textdaten dem Anwendungsfall gerecht sein sollten. Zudem ist festzuhalten, dass sich die Sprache und die Domäne der \ac{ATS} über die im jeweils erforderlichen Fine-Tuning zugeführten Textdaten steuern lässt.\\

\noindent
Möglich sind außerdem Ansätze wie die sogenannte Data Augmentation oder auch die Übersetzung anderssprachiger Texte, um künstlich neue Textdaten für den deutschsprachigen Raum zu erzeugen. Hier ist die entstehende Qualität der Textdaten in jedem Fall mit höchster Vorsicht zu untersuchen. Um also weitergehende Fortschritte in der deutschsprachigen \ac{ATS} zu erzielen, sind verschiedenartige Projekte, welche sich ausschließlich auf die Formation neuer Korpora konzentrieren, unabdingbar.\\

\noindent
Das Pareto-Prinzip kann indes vom Menschen auf die Maschine übertragen werden und knüpft hier unmittelbar an. Demnach erfolgen 80\% der Arbeit in nur 20\% der Zeit. Die letzten 20\% der Arbeit bedürfen folglich ganze 80\% der Zeit. Hier passieren allerdings ganz entscheidende Dinge. Dies bedeutet, dass das \ac{TF2TF} mit fortwährender Trainingsdauer etliche Feinheiten der zugeführten Textdaten erlernt, welche sich erheblich auf die Qualität auswirken. Gleichzeitig lassen sich entsprechende Schwächen der generierten Zusammenfassungen auf eben diese fehlende Phase zurückführen, welche aus unzureichenden Textdaten resultiert. Eine umfangreichere Datengrundlage, bestenfalls mit einem Umfang im Millionenbereich, ist daher umso wichtiger.\\

\noindent
Darüber hinaus ist es je nach Anwendungsfall beispielsweise sinnvoll, das Verhalten der \ac{ATS} unter Zugabe kurzer und langer Texte zu analysieren. Anpassungen lassen sich bei Bedarf mit einem sogenannten Sliding Window Approach, welcher das methodische Teilen und Konkatenieren der Texte vorsieht, vornehmen. Hier sei angemerkt, dass HuggingFace bereits entsprechende Methoden mitliefert und daher immerhin strukturbezogene Exploitation vorbeugt.\\

\noindent
Im Verlauf der Experimente fielen zudem wiederholt ungewöhnliche \ac{ROUGE}-Scores auf. Zumeist entstammt auch dieses Phänomen den Eigenarten der zugrundeliegenden Textdaten. Dennoch sind geeignete Metriken zu erforschen und auszuwählen, welche es im Kontext der \ac{ATS} ermöglichen, das Training vor einem potenziellen Overfitting zu stoppen. Dies würde jedoch folglich eine äußerst komplizierte Messung des Generalisierungseffektes bedeuten und ist daher eine wesentliche Herausforderung, deren Bewältigung einen ähnlich großen Durchbruch in der Wissenschaft der \ac{ATS} bewirken könnte, wie es die vortrainierten \ac{DLR} taten.\\

\noindent
Weiterhin ist die Optimierung der Hyperparameter notwendig. Zwar ist dies bei extern vortrainierten Modellen wie den \ac{DLR} nur eingeschränkt möglich, dafür bietet \ac{TF2TF} entsprechende Konfigurationsmöglichkeiten. Die Hyperparameter sind entweder vor oder adaptiv während des Trainings zu definieren. Meist sind die Ergebnisse bereits nach der Zugabe von 10-20\% der eigentlichen Daten repräsentativ und somit vergleichbar, weshalb die Optimierung keine unzähligen vollständigen Trainingsläufe erfordert. Neben der \ac{LR} und der Batch Size ist unter anderem auch die Anzahl der Aufwärmschritte, die Anzahl der verborgenen Schichten und die Anzahl der Attention-Heads zu bestimmen. Darüber hinaus können verschiedene Parameter des verwendeten Tokenizers konfiguriert werden. Nicht zuletzt ist das Teilen der Gewichte zwischen dem Encoder und dem Decoder zu erproben. In der verwendeten Konfiguration des \ac{TF2TF} werden Texte mit einer Länge von 512 Token akzeptiert, während entsprechende Zusammenfassungen auf 128 Token limitiert sind. Dies entspricht einer Kompressionsrate von 75\%. Die Natur der entstehenden \ac{ATS} ist nicht nur über diese Werte, sondern abermals über die zugeführten Textdaten bestimmbar. Somit lässt sich erwartungsgemäß je nach Anwendungsfall auch die Kompressionsrate bedarfsgerecht steuern \cite[S.~14-15]{NIT19}.\\

\noindent
In Bezugnahme auf die zu Beginn konstruierten Einsatzgebiete aus dem Gesundheitswesen lassen sich folgende Maßnahmen ableiten: Die Datengrundlage ist mit umfangreichen medizinischen Texten zu erweitern, bevor ein entsprechendes Fine-Tuning mit anschließender Optimierung der Hyperparameter erfolgen kann. Es existieren hierbei jedoch neue domänenspezifische Gefahren. Demnach gestaltet sich insbesondere die Zusammenfassung von Patientengesprächen als höchst kompliziert, weil initial bereits eine hohe Informationsdichte vorliegt und somit wahrscheinlich ein gewisser Informationsverlust entsteht.
