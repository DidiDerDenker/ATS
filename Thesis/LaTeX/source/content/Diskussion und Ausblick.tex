\chapter{Diskussion und Ausblick}
\thispagestyle{fancy}
\label{chap:Diskussion und Ausblick}

% TODO: Adaptive Learning für die Modelle ansatzweise vorstellen, Modell auf Dialogcharakter adaptieren, um es in der Verdichtung von Protokollen einer Videosprechstunde zu nutzen, bzw. generell bspw. Meetings zusammenzufassen, Forschungsstand und SOTA-Modelle hierfür im einleitenden Kapitel beschreiben, hier aufgreifen (vgl. Paper: „Abstractive Dialogue Summarization with Sentence-Gated Modeling Optimized by Dialogue Acts“ und “Using a KG-Copy Network for Non-Goal Oriented Dialogues”), bereits Architekturen vorstellen (vgl. „Automatic Dialogue Summary Generation of Customer Service“ und „Dialogue Response Generation using Neural Networks and Background Knowledge“ und „Global Summarization of Medical Dialogue by Exploiting Local Structures”), Modell ohne Anpassungen auf Konversationen anwenden: \url{https://www.isi.edu/natural-language/people/hovy/papers/05ACL-email_thread_summ.pdf}, Modell nutzen, um Zusammenfassungen für Texte zu generieren und damit neue Datensätze für neue Modelle zu generieren, aber stark von der Qualität abhängig, gelb markierte Literatur sichten und verwenden, Datumsangaben aktualisieren, Ausblick: Zusammenfassungen formatieren, also Großschreibung nach Satzenden oder auch Leerzeichenentfernung vor Punkten, Adaption auf Sprache und/ oder Domain, Ausblick: Ausblick: Datengrundlage besteht aus frei verfügbaren allgemeinsprachlichen, ausreichend langen und deutschsprachigen Daten, verschiedene Herkünfte, auf Grundlage dieser Allgemeinsprache und den eben genannten Vorhaben, sollte ein grundlegendes Modell trainiert werden und später für den Use Case eine Art Adaptive Learning betrieben werden, d.h. wenn bekannt ist, dass das Modell für medizinische Texte angewandt werden soll, sollte man vorher die Parameter des Modells finetunen, Gefahr: Initial bereits hohe Informationsdichte bei Diktaten - "Was fällt raus?", Ausblick: Limitations von NLP: \cite[S.~30-31]{BIR09}, Fine-Tuning in Zieldomäne notwendig, Analysen zum Zeitverhalten und zum Verhalten mit kurzen und langen Texten analysieren, Fazit: BART von einem der größten Unternehmer der Branchen vortrainiert, immer noch nicht perfekt, Fortschritte erkennbar, auch multilinguale Adaptivität, dennoch selbst trainierter BERT bis auf wenige strukturelle Schwächen fast genauso gut, zukünftig mehr Aufmerksamkeit auf die Kollektion von Daten notwendig, Modell lernt datenentsprechend, bei Einsatz ist Fokus auf umfangreichen Korpus notwendig, Pareto-Prinzip für Maschine wie für Menschen auch, Dauer Empfehlung > 1 Mio. Daten, weil hinten raus viel Detailarbeit passiert
