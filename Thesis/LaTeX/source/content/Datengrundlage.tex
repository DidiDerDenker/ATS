\chapter{Datengrundlage}
\thispagestyle{fancy}
\label{chap:Datengrundlage}

Notizen:
\begin{itemize}
	\item Modelle erfordern keine gelabelten Daten, wohl aber gesichtete Daten	
	\item Siehe Abstract im Exposé
\end{itemize}


\section{Akquise}
Notizen:
	\item Akquise mittels Skripten in Python
	\item Zielform der Textdateien beschreiben, Ablagestruktur ebenfalls
	\item Datenquellen: Wikipedia-API (\url{https://pypi.org/project/Wikipedia-API/}, rekursiv), OpenLegalData-Dumps (\url{https://de.openlegaldata.io/pages/api/}, \url{https://static.openlegaldata.io/dumps/de/2019-10-21/}), CORPUS (\url{https://archive.ics.uci.edu/ml/datasets/Legal+Case+Reports}), ENROS (\url{https://www.cs.cmu.edu/~./enron/}), MENDELEY (\url{https://data.mendeley.com/datasets/zm33cdndxs/2})
	\item Datenherkünfte beschreiben, d.h. Dateiformat, Größe, Sprache etc. beschreiben
	\item Nicht-deutschsprachige Texte werden übersetzt und geprüft
	\item Von den Datenquellen wird vermutet und nach manueller Einsicht bestätigt, dass Texte dort grammatikalisch korrekt sind, außerdem allgemeinsprachlich und ausreichend lang (> 1000 Wörter, es wird angenommen, dass 1000 Wörtern vorliegen müssen, um eine Zusammenfassung erforderlich zu machen) sind und möglichst diversifizierten Themengebieten entstammen


\section{Vorverarbeitung}
Notizen:
	\item Daten iterieren und vorverarbeitet abspeichern, Ausführung der NLP-Pipeline erwähnen, ebenfalls für tokenisierte und lemmatisierte Versionen, d.h. später nur noch Durchlauf eines bestimmten Ordners, zuvor alle möglichen Dateien sichten und möglichst viele Fehler im Laufe des erneuten Exportes eliminieren, Ablageorte und Textdateiversionen beschreiben
	\item Weitergehende Besonderheiten innerhalb der Texte werden toleriert, da diese auch im Praxisbetrieb auftreten könnten und somit gekannt werden sollten
	\item Möglicherweise Spell Checking von Google RS für die deutsche Sprache einbinden


\section{Datensatz}
Notizen:
	\item Datengrundlage besteht aus frei verfügbaren allgemeinsprachlichen, ausreichend langen und deutschsprachigen Daten, verschiedene Herkünfte
	\item Auf Grundlage dieser Allgemeinsprache und den eben genannten Vorhaben, sollte ein grundlegendes Modell trainiert werden und später für den Use Case eine Art Adaptive Learning betrieben werden, d.h. wenn bekannt ist, dass das Modell für medizinische Texte angewandt werden soll, sollte man vorher die Parameter des Modells finetunen
	\item Später dann zwecks Adaption auch unternehmensinterne fachspezifische Daten notwendig, genauer beschreiben, perspektivisch sogar fachspezifische, dialogorientierte oder auch mehrsprachige Modelle möglich, dementsprechend mehr Daten benötigt, ggf. erst im Ausblick erwähnen
	\item Skript zum Einlesen entwickeln, bspw. "data_loader"
