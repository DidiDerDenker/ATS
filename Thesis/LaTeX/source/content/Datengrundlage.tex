\chapter{Datengrundlage}
\thispagestyle{fancy}
\label{chap:Datengrundlage}

\noindent
Um die Ziele dieser Arbeit zu erreichen, ist die Entwicklung theoretisch analysierter Architekturen zur \ac{ATS} und zur sprachtechnischen Adaption erforderlich. Hierfür ist eine geeignete Datengrundlage bereitzustellen, welche insbesondere Qualität, aber auch Vergleichbarkeit der entsprechenden Modelle ermöglicht. Fortan wird die Datengrundlage als Korpus $K$ bezeichnet, wobei dieser Korpus aus verschiedenen Datensätzen $d_i$ besteht, also $$K=\begin{pmatrix} d_1 \\ \vdots \\ d_n \end{pmatrix}$$ für $i=1,...,n$ mit möglichst großem $n$ hinsichtlich hoher Qualität. Die Datensätze, welche den gesuchten Korpus bilden, müssen dabei bestimmten Anforderungen genügen. Ihnen wird insbesondere eine paarweise Natur abverlangt. Für $d_i \in K$ und $i=1,...,n$ gilt also: $d_i=\{t_i,s_i\}$. Neben dem ursprünglichen Text $t_i$ ist hier eine Zusammenfassung $s_i$ gefordert, welche als Referenz für die modellseitig zu generierende Zusammenfassung dient. Nur so ist die Qualität messbar und der Lernfortschritt realisierbar. Aufgrund der explorativen Natur dieser Arbeit werden sowohl englisch- als auch deutschsprachige Datensätze benötigt, wobei deren zugrundeliegende Domäne zunächst nicht von hoher Relevanz ist. Die Länge der Texte und der Zusammenfassungen haben einen hohen Einfluss darauf, wie das trainierte Modell die eigenen Zusammenfassungen generieren wird. Zwar wird hierfür keine Mindestlänge definiert, dennoch seien folgende Richtwerte gegeben: Texte $t_i$ sollten aus mindestens 200 Wörtern bestehen. Zusammenfassungen $s_i$ hingegen sollten einige Sätze vorweisen können. Alle Texte und Zusammenfassungen sollten darüber hinaus zwischen Klein- und Großschreibung unterscheiden.\\

\noindent
Unter Berücksichtigung obiger Anforderungen werden nun vier Korpora ausgewählt. Diese werden wie folgt deklariert und nachfolgend beschrieben: $K_{eng}$, $K_{wik}$, $K_{nws}$, $K_{mls}$. Der Korpus $K_{eng}$ dient als initialer Trainingskorpus und besteht aus etwa 300.000 englischsprachigen Datensätzen. Er wurde von TensorFlow verarbeitet und veröffentlicht, entstammt allerdings ursprünglich der CNN und der DailyMail \cite{TEN21}.\\

\noindent
Aufgrund der nachrichtenorientierten Domäne ist von stark variierenden Textinhalten auszugehen. Dies verspricht zunächst einen hohen generalisierenden Effekt, wobei individuelle Zieldomänen womöglich andere Eigenarten aufweisen und mitunter eine andere Beschaffenheit des Korpus erfordern. Dies ist allerdings nicht Teil dieser Arbeit und gilt lediglich als sensibilisierende Anmerkung. Die Eignung des Korpus wird insbesondere durch die weitreichende Nutzung in der Wissenschaft bestärkt, denn \ac{SOTA}-Modelle werden oftmals auf diesem Korpus verglichen. Texte dieses Korpus bestehen durchschnittlich aus etwa 850 Wörtern, Zusammenfassungen hingegen aus etwa 60 Wörtern. Dies spricht für einen hohen Abstraktionsgrad und damit eine hohe Verdichtung \cite[S.~6]{ROT20}.\\

\noindent
Die anderen drei Korpora dienen dem weitergehenden Training und bestehen demzufolge aus deutschsprachigen Datensätzen. Der Korpus $K_{wik}$ wurde 2019 im Kontext der Swiss Text Analytics Conference als Grundlage eines Wettbewerbs publiziert und umfasst 100.000 Datensätze \cite{CIE19}. Die Textinhalte entstammen der deutschsprachigen Wikipedia, weshalb auch hier von einer vielfältigen Domäne auszugehen ist. Der Korpus $K_{nws}$ wurde durch einen in Python selbst entwickelten Crawler generiert. In einer Zeitspanne von über zehn Monaten wurden mehr als 50.000 Nachrichtenartikel automatisiert kollektiert \cite[S.~79,~83,~416]{BIR09}. Diese entstammen den folgenden Nachrichtenagenturen: SÜDDEUTSCHE ZEITUNG, DER SPIEGEL, ZEIT ONLINE. Nach Sichtung der verfügbaren Daten können nur die Artikel der ZEIT ONLINE als geeignet bewertet werden. Die Texte der anderen beiden Agenturen lassen sich technisch nicht erfassen. Folglich sind etwa 15.000 Datensätze nutzbar. Der Korpus $K_{mls}$ nennt sich MLSUM und steht als multilingualer Korpus für das Training der \ac{ATS} zur Verfügung. Die darin enthaltenen über 200.000 deutschsprachigen Datensätze werden a priori extrahiert und entstammen erneut einer vornehmlich nachrichtenorientierten Domäne \cite{SCI20}.\\

\noindent
Texte der deutschsprachigen Korpora bestehen durchschnittlich aus etwa 4.000 Wörtern, Zusammenfassungen hingegen aus etwa 250 Wörtern.\\

\noindent
Üblicherweise existiert beim anvisierten Training die Gefahr der sogenannten Exploitation. Diese Gefahr meint im Kontext der \ac{ATS} konkret, dass das zugrundeliegende Modell die Struktur der Artikel anstatt der Inhalte der Artikel lernt \cite[S.~476]{GOO16}. Grund für diese Annahme ist der typische Aufbau von Wikipedia-Artikeln. Diese beinhalten zumeist bereits im ersten Absatz stark verdichtete Informationen, also eine Art Zusammenfassung. Dies macht eine hohe Anzahl an Trainingsdaten verschiedener Herkunft erforderlich. Die ersten Absätze der Wikipedia-Artikel werden bereits während des Ladeprozesses ignoriert. Dennoch sollte zur Vorbeugung stets eine Mischung aus den drei deutschsprachigen Korpora vorgenommen werden \cite[S.~42]{BIR09}.\\

\noindent
Um insbesondere der Kritik an den präsentierten Metriken entgegenzuwirken und belastbare Aussagen treffen zu können, werden nun ein englisch- und ein deutschsprachiger Korpus eingeführt. Diese dienen der qualitativen Analyse maschinell generierter Zusammenfassungen (siehe Anhang A und B). Dabei ist strukturell und inhaltlich von korpusübergreifender Gleichheit auszugehen, während die beinhalteten Texte korpusintern möglichst unterschiedlich sind. Neben Berichten und einer Definition ist beispielsweise auch ein Rechtsurteil und eine Anleitung enthalten. Die Texte können hierbei der Allgemeinsprache, der Fachsprache und der Alltagssprache zugeordnet werden. Die Texte entstammen bundesweiten Informations- sowie Nachrichtenkanälen und wurden entsprechend übersetzt.
