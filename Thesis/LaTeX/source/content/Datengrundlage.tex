\chapter{Datengrundlage}
\thispagestyle{fancy}
\label{chap:Datengrundlage}

\noindent
Um die Ziele dieser Arbeit zu erreichen, ist die Entwicklung theoretisch analysierter Architekturen zur \ac{ATS} und zur sprachtechnischen Adaption erforderlich. Hierfür ist eine geeignete Datengrundlage bereitzustellen, welche insbesondere Qualität, aber auch Vergleichbarkeit der entsprechenden Modelle ermöglicht. Fortan wird die Datengrundlage als Korpus $K$ bezeichnet, wobei dieser Korpus aus verschiedenen Datensätzen $d_i$ besteht, also $$K=\begin{pmatrix} d_1 \\ \vdots \\ d_n \end{pmatrix}$$ für $i=1,...,n$ mit möglichst großem $n$ hinsichtlich hoher Qualität. Die Datensätze, welche den gesuchten Korpus bilden, müssen dabei bestimmten Anforderungen genügen. Ihnen wird insbesondere eine paarweise Natur abverlangt. Für $d_i \in K$ und $i=1,...,n$ gilt also: $d_i=\{t_i,s_i\}$. Neben dem ursprünglichen Text $t_i$ ist hier eine Zusammenfassung $s_i$ gefordert, welche als Referenz für die modellseitig zu generierende Zusammenfassung dient. Nur so ist die Qualität messbar und der Lernfortschritt realisierbar. Aufgrund der explorativen Natur dieser Arbeit werden sowohl englischsprachige als auch deutschsprachige Datensätze benötigt, wobei deren zugrundeliegende Domäne zunächst nicht von hoher Relevanz ist. Die Länge der Texte und der Zusammenfassungen haben einen hohen Einfluss darauf, wie das trainierte Modell die eigenen Zusammenfassungen generieren wird. Zwar wird hierfür keine Mindestlänge definiert, dennoch seien folgende Richtwerte gegeben: Texte $t_i$ sollten aus mindestens 200 Wörtern bestehen. Zusammenfassungen $s_i$ hingegen sollten einige Sätze vorweisen können. Alle Texte und Zusammenfassungen sollten darüber hinaus zwischen Klein- und Großschreibung unterscheiden.\\

\noindent
Unter Berücksichtigung obiger Anforderungen werden nun vier Korpora ausgewählt. Diese werden wie folgt deklariert und nachfolgend beschrieben: $K_a$, $K_b$, $K_c$, $K_d$. Der Korpus $K_a$ dient als initialer Trainingskorpus und besteht aus etwa 300.000 englischsprachigen Datensätzen. Er wurde von TensorFlow verarbeitet und veröffentlicht, entstammt allerdings ursprünglich der CNN und der DailyMail \cite{TEN21}. Aufgrund der nachrichtenorientierten Domäne ist von stark variierenden Textinhalten auszugehen. Dies verspricht zunächst einen hohen generalisierenden Effekt, wobei individuelle Zieldomänen womöglich andere Eigenarten aufweisen und mitunter eine andere Beschaffenheit des Korpus erfordern. Dies ist allerdings nicht Teil dieser Arbeit und gilt lediglich als sensibilisierende Anmerkung. Die Eignung des Korpus wird insbesondere durch die weitreichende Nutzung in der Wissenschaft bestärkt, denn \ac{SOTA}-Modelle werden oftmals auf diesem Korpus verglichen. Texte dieses Korpus bestehen durchschnittlich aus etwa 850 Wörtern, Zusammenfassungen hingegen aus etwa 60 Wörtern. Dies spricht für einen hohen Abstraktionsgrad und damit eine hohe Verdichtung \cite[S.~6]{ROT20}.\\

\noindent
Die anderen drei Korpora dienen dem weitergehenden Training und bestehen demzufolge aus deutschsprachigen Datensätzen. Der Korpus $K_b$ wurde 2019 im Kontext der Swiss Text Analytics Conference als Grundlage eines Wettbewerbes publiziert und umfasst 100.000 Datensätze \cite{CIE19}. Die Textinhalte entstammen der deutschsprachigen Wikipedia, weshalb auch hier von einer vielfältigen Domäne auszugehen ist. Der Korpus $K_c$ wurde durch einen in Python selbst entwickelten Crawler generiert. In einer Zeitspanne von sechs Monaten wurden mehr als 50.000 Nachrichtenartikel automatisiert kollektiert \cite[S.~79,~83,~416]{BIR09}. Nach Sichtung der verfügbaren Daten können Artikel der ZEIT ONLINE als geeignet bewertet werden. Demnach sind etwa 15.000 Datensätze nutzbar. Der Korpus $K_d$ nennt sich MLSUM und steht als multilingualer Korpus für das Training der \ac{ATS} zur Verfügung. Die darin enthaltenen über 200.000 deutschsprachigen Datensätzen entstammen erneut vornehmlich einer nachrichtenorientierten Domäne \cite{SCI20}.\\

\noindent
Texte der deutschsprachigen Korpora bestehen durchschnittlich aus etwa 4.000 Wörtern, Zusammenfassungen hingegen aus etwa 250 Wörtern. Üblicherweise existiert beim anvisierten Training die Gefahr der sogenannten Exploitation. Diese Gefahr meint im Kontext der \ac{ATS} konkret, dass das zugrundeliegende Modell die Struktur der Artikel anstatt der Inhalte der Artikel lernt. Grund für diese Annahme ist der typische Aufbau von Wikipedia-Artikeln. Diese beinhalten zumeist bereits im ersten Absatz stark verdichtete Informationen, also eine Art Zusammenfassung. Dies macht eine hohe Anzahl an Trainingsdaten verschiedener Herkunft erforderlich. Die ersten Absätze der Wikipedia-Artikel werden beim Laden bereits ignoriert. Dennoch sollte zur Vorbeugung stets eine Mischung aus den drei deutschsprachigen Korpora vorgenommen werden \cite[S.~42]{BIR09}.
\newpage

\noindent
Um insbesondere der Kritik an den präsentierten Metriken entgegenzuwirken und belastbare Aussagen treffen zu können, wird nun ein englisch- und ein deutschsprachiger Korpus eingeführt, welche der qualitativen Analyse maschinell generierter Zusammenfassungen dienen (siehe Anhang A und B). Dabei ist strukturell und inhaltlich von korpusübergreifender Gleichheit auszugehen, während die beinhalteten Texte korpusintern möglichst unterschiedlich sind. Neben Berichten und Definitionen ist beispielsweise auch ein Rechtsurteil und eine Gebrauchsanweisung enthalten. Die Texte können hierbei der Allgemeinsprache, der Fachsprache und der Alltagssprache zugeordnet werden. Die Texte entstammen bundesweiten Informations- und Nachrichtenkanälen.
