\chapter{Einleitung}
\thispagestyle{fancy}
\label{chap:Einleitung}
\pagenumbering{arabic}

Notizen \cite{BAC15}:
\begin{itemize}
	\item Zweck der Automatic Text Summarization beschreiben
	\item Anwendungsgebiete: Report-Generierung, Nachrichten-Zusammenfassung, Überschriften-Generierung (vgl. Paper: „Automatic Text Summarization with Machine Learning“), dadurch Reduktion der Lesezeit oder auch Entscheidungsunterstützung
	\item Kontext und Notwendigkeit der Arbeit im Gesundheitswesen offenlegen
	\item Praktischen Workflow bzw. Integration dieser Arbeit in die Praxis beschreiben
	\item Szenarien: Spracherkennung und Sprechererkennung auf Aufzeichnungen einer Videosprechstunde anwenden, Modelle dieser Arbeit dann für die Verdichtung der entstehenden Protokolle integrieren, Multi-Dokument-Zusammenfassung, dialogorientierte Zusammenfassungen
	\item Text Summarization bspw. auch als Mix aus Entity Recognition und Text Generation
	\item Siehe Abstract im Exposé
\end{itemize}


\section{MediaInterface GmbH \& SpeaKING\textsuperscript{\textregistered}}
Notizen:
\begin{itemize}
	\item Unternehmen beschreiben
	\item Produkt beschreiben
\end{itemize}


\section{Zielsetzung}
Notizen:
\begin{itemize}
	\item Ziele definieren („Automatic Single Document Summarization“, extraktiven und abstraktiven Ansatz definieren, in dieser Arbeit noch keine Zusammenfassung von Texten mit Dialogcharakter)
	\item Forschungsfragen formulieren
\end{itemize}


\section{Aufbau der Arbeit}
Notizen:
\begin{itemize}
	\item Kapitel beschreiben
	\item Grafik als roten Faden dieser Arbeit skizzieren
\end{itemize}


\section{Forschungsstand \& Referenzen}
Notizen:
\begin{itemize}
	\item NLP-SOTA beschreiben, ggf. Übergriff zu anderen interdisziplinären Anwendungsgebieten andeuten, hier genutzte Datensätze, welche als Benchmark dienen und die zur Verbesserung des Scores genutzt werden könnten, sind: \url{https://paperswithcode.com/task/abstractive-text-summarization}, \url{https://paperswithcode.com/task/text-summarization}, also CNN/ Daily-Mail, Wikihow und Gigaword
	\item Vergleichbare Arbeiten beschreiben (vgl. Paper: „German Abstractive Text Summarization using Deep Learning“ und „Automatic Text Summarization“)
	\item Kürzlich entwickelte Ansätze (vgl. Paper: „Recent Automatic Text Summarization Techniques“)
	\item SOTA-Modelle recherchieren (vgl. Paper: „Automatic Text Summarization“ und weitere Architekturen aus dem Internet, ggf. auch ohne zugehöriges Paper)
	\item Nützliche GitHub-Repo's verlinken/ referenzieren und deren hauptsächliche Herangehensweise dokumentieren, hierfür siehe GitHub-Stars und Notizen nachfolgender Kapitel
	\item Medizinische Zusammenfassung: \url{https://github.com/armancohan/long-summarization}
	\item Architekturen: \url{https://towardsdatascience.com/deep-learning-models-for-automatic-summarization-4c2b89f2a9ea}, \url{https://medium.com/analytics-vidhya/deep-reinforcement-learning-deeprl-for-abstractive-text-summarization-made-easy-tutorial-9-c6914999c76c}, \url{https://github.com/yaserkl/RLSeq2Seq#dataset}, \url{https://medium.com/analytics-vidhya/deep-reinforcement-learning-deeprl-for-abstractive-text-summarization-made-easy-tutorial-9-c6914999c76c}, \url{https://github.com/rohithreddy024/Text-Summarizer-Pytorch}, \url{https://github.com/oceanypt/A-DEEP-REINFORCED-MODEL-FOR-ABSTRACTIVE-SUMMARIZATION}, \url{https://github.com/theamrzaki/text_summurization_abstractive_methods}
	\item Möglicherweise als Paper aufnehmen, oder sogar für spätere Kapitel nutzen: \url{https://arxiv.org/abs/1805.11080, https://arxiv.org/pdf/1705.04304v3.pdf}, gleiche Prüfung stets auch bei andere URL's in den Notizen dieser Arbeit durchführen
	\item Vergleich verschiedener Modelle anhand des ROUGE-Scores: \url{http://nlpprogress.com/english/summarization.html}, in den Ergebnissen erwähnen, Korpus-Zusammensetzung beachten
\end{itemize}
