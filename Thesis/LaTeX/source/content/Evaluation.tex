\chapter{Evaluation}
\thispagestyle{fancy}
\label{chap:Evaluation}

\noindent
In diesem Kapitel werden die zuvor konzipierten Experimente umfangreich evaluiert. Dies geschieht einerseits auf Basis der ausgewählten Metriken, andererseits in Form einer qualitativen Analyse der aus den Anhängen A und B generierten Zusammenfassungen.\\

\noindent
\textbf{[EXP-01]}\\
\noindent

\noindent
\textbf{[EXP-02]}\\
\noindent

\noindent
\textbf{[EXP-03]}\\
\noindent

+ Empfehlung pro Experimente/ pro Sprache/ pro Modell, bspw. bzgl. Domäneneignung, Kritik

BERT-EN:

Das entsprechende Training verlief komplikationslos und mit der erwarteten Fehlerabnahme. Letztlich sind die folgenden \ac{ROUGE}-Scores zu verzeichnen: R-Recall: 15.78, R-Precision: 10.25, R-Measure: 12.11. Diese stimmen unter Akzeptanz womöglich unbekannter Nebenbedingungen mit den umworbenen \ac{ROUGE}-Scores des \ac{SOTA} überein.\\

\noindent
Zudem wird der in Anhang A befindliche englischsprachige Text mithilfe des trainierten Modells zusammengefasst. Der Text unterscheidet sich strukturell von den eingegangenen Trainingsdaten und weist etwas über 1.000 Token auf. Weiterhin entstammt er einer dem Modell unbekannten Domäne. Fachlich ist er zugleich nicht zu spezifisch. Er handelt von den Anschlägen des 11. Septembers 2001 in New York. Die entstehende Zusammenfassung ist nachfolgend einzusehen. Es ist aus menschlich subjektiver Sicht erkennbar, dass nicht nur die verdichteten Informationen, sondern auch die Grammatik und die Orthographie weitestgehend, aber nicht vollumfänglich, korrekt sind.\\

\noindent\fbox{%
\parbox{\textwidth}{%
Firefighters, police officers, search-and-rescue dogs headed to Ground Zero to search for survivors. They didn't know how many people were trapped alive in the wreckage. They were able to search through the unstable piles of rubble for air pockets. By September 11th, workers had rescued all the people trapped at the site.
}%
}\\


BERT-DE:

In einem initialen Experiment wird die multilinguale Version von \ac{BERT} mit einem Mix aus allen verfügbaren deutschsprachigen Daten, also letztlich monolingual, trainiert. Hierbei entstehen die folgenden \ac{ROUGE}-Scores: R-Recall: 15.93, R-Precision: 10.52, R-Measure: 11.97. Dies entspricht dem \ac{SOTA}, wenn man anhand der vorliegenden Werte urteilt. Nachfolgend sei die fehlerbehaftete Zusammenfassung des in Anhang B einzusehenden deutschsprachigen Textes gezeigt. Dieser gleicht strukturell sowie inhaltlich weitestgehend dem bereits beschriebenen Text in Anhang A.\\

\noindent\fbox{%
\parbox{\textwidth}{%
Der 11. September 2001 war der größte Unfall in der Geschichte der USA. Doch die Bilder zeigen, wie groß die Welt ist, und wie groß der Druck auf die Menschen ist. 500 Feuerwehrleute und Polizeibeamte starben. New York steht unter Schock und vor einem Desaster für die USA. Ein Gedenkpavillon erinnert an die Aufräumarbeiten auf dem World Trade Center.
}%
}\\

\noindent
Die inhaltliche Insuffizienz ist für den menschlichen Leser auch ohne Kenntnis über den Originaltext unschwer erkennbar. Dies ist aus praktischer Sicht zwar nachteilig, ist aber auch im englischsprachigen \ac{SOTA} existent, wenn auch subjektiv weniger gravierend. Die Grammatik und die Orthographie hingegen scheinen fehlerfrei zu sein.\\

\noindent
Testweise wird nun die multilinguale Version von \ac{BERT} durch eine eigens für die deutsche Sprache vortrainierte Version ausgetauscht. Hierbei handelt es sich um den sogenannten \ac{GBERT}, welcher von Deepset AI, einem globalen Anbieter von Open-Source-Lösungen für \ac{NLP}, vortrainiert und bereitgestellt wird. An der deutschsprachigen Datengrundlage wird zunächst nichts verändert. Hierbei entstehen die folgenden \ac{ROUGE}-Scores: R-Recall: 17.83, R-Precision: 10.79, R-Measure: 12.72. Nachfolgend ist erneut eine Zusammenfassung des Textes aus Anhang B zu sehen.\\

\noindent\fbox{%
\parbox{\textwidth}{%
Der 11. September 2001 war der erste Terroranschlag in der Geschichte der USA. Die Bilder der Katastrophe zeigen, wie sehr die USA sich auf die Welt vorbereiten - und wie sich die Welt verändert hat. Das World Trade Center ist ein Symbol für die Zerstörung der Welt. Es ist ein Ort der Trauer und der Trauer. Das Museum zeigt die Ereignisse des 11. Septembers.
}%
}\\
\newline

\noindent
Obwohl die \ac{ROUGE}-Scores den \ac{SOTA} übersteigen, ist die exemplarische Zusammenfassung inhaltlich noch immer sehr fehlerbehaftet. Daher werden in einem folgenden Experiment die weitergehenden Potenziale der multilingualen Version von \ac{BERT} erprobt. Hierfür werden nun im Trainingsprozess alle verfügbaren Daten, bestehend aus englisch- und deutschsprachigen Texten, unter sonst gleichen Bedingungen berücksichtigt. Dabei entstehen die folgenden \ac{ROUGE}-Scores: R-Recall: 16.76, R-Precision: 11.18, R-Measure: 12.68. Es ist festzuhalten, dass das Training auf einer multilingualen Datenbasis sowohl wertmäßig als auch subjektiv qualitativ bessere Ergebnisse als das Training auf einer monolingualen Datenbasis hervorbringt. Nachfolgend wird dies anhand der Zusammenfassung des Anhang B deutlich.\\

\noindent\fbox{%
\parbox{\textwidth}{%
Der 11. September 2001 war der größte Unfall in der Geschichte der Welt. Doch die Bilder, die an diesem Mittwoch in New York passierten, sind fassungslos. Fast 400 Feuerwehrleute und Polizeibeamte verloren bei den Rettungsarbeiten ihr Leben. Der traurige Tag ging fortan als 9/11 in die Geschichte ein.
}%
}\\
\newline

\noindent
Trotz allen bisherigen Fortschritten sind weiterhin inhaltliche Mängel in den generierten Zusammenfassungen zu erkennen. Mit dem Wissen, dass das Training auf einer multilingualen Datenbasis bessere Ergebnisse hervorbringt, wird nun \ac{BERT} durch \ac{XLM-R} ausgetauscht und unter sonst gleichen Bedingungen trainiert. Das Training basiert folglich ebenfalls auf allen verfügbaren Daten, strebt die vermuteten Potenziale von \ac{XLM-R} an und bringt die folgenden \ac{ROUGE}-Scores hervor: R-Recall: 0.00, R-Precision: 0.00, R-Measure: 0.00. Nachfolgend ist die Zusammenfassung des Textes aus Anhang B zu sehen.\\

\noindent\fbox{%
\parbox{\textwidth}{%
...
}%
}\\
\newline
 
% TODO: BERT durch XLM-R austauschen, ggf. BART, sonst Nichtnutzung begründen

% TODO: Bewährtes Modell auswählen, zu Optimierungszwecken weiter experimentieren

% TODO: Übersetzungen, Data Augmentation und Sliding Window Approach in den Ausblick verschieben, Texte untenstehend sukzessive geraderücken und komplettieren, profitieren multilinguale Modelle von dem verborgenen Strukturen anderer Sprachen?


\noindent
Trotz der monolingualen Fortschritte wird \ac{GBERT} zunächst durch \ac{XLM-R} und anschließend durch \ac{BART} ersetzt, um sich weiterhin geeigneten Ergebnissen zu nähern und multilinguale Modelle zu erproben. Der Tokenizer wird ebenfalls entsprechend ausgetauscht. Es entstehen die folgenden \ac{ROUGE}-Scores: R-Recall: 23.02, R-Precision: 13.95, R-Measure: 16.09. Obgleich \ac{XLM-R} bewiesenermaßen verschiedene \ac{NLP}-Aufgaben begünstigt, kann sie in der vorliegenden Konfiguration keine Verbesserung der \ac{ATS} erzielen. Dies basiert nicht nur auf den verringerten \ac{ROUGE}-Scores, sondern auch auf der nachfolgenden exemplarischen Zusammenfassung von Anhang B, welche inhaltlich subjektiv als mangelhaft eingestuft werden kann.\\

\noindent\fbox{%
\parbox{\textwidth}{%
Der 11. Flugtag der Weltturm-Weltmeisterschaft 2001 fand am 11. September 2001 in New York City statt und war das erste Mal in der Geschichte des World Trade Centers. Das National September 11 Memorial and Museum in Manhattan ist ein historischer Gedenkpavillon in Manhattan. Es befindet sich in der Nähe des World Trade Centers in Manhattan.
}%
}\\

% TODO: BART trainieren und kritisieren, evtl. ist GBERT trotzdem besser

\noindent
In der Konsequenz wird \ac{GBERT} für die weiteren Experimente genutzt. Hierbei werden insbesondere die Anteile der jeweiligen Korpora an den Trainings- und Testdaten variiert, um vornehmlich dem Lernen der Textstruktur entgegenzuwirken. Die Konfigurationen sowie entsprechende Ergebnisse sind der Tabelle ? zu entnehmen.\\

% TODO: Tabelle ? nach erfolgten Trainingsschritten erstellen, oben referenzieren und Erkenntnisse beschreiben/ herausstellen, weitere Schritte begründet anschließen, bspw. Data Augmentation, Übersetzung englischsprachiger Korpora, Parts im Example überarbeiten, Experimente im Excel kompaktieren, bspw. letztes Tabellenblatt

\noindent
- Training auf Wiki, Evaluation auf News
  -> Sehr schlecht, strukturerlernend
- Training auf News mit ein bisschen Wiki, Evaluation mit New
   -> Positiver Effekt, aber zu wenig Daten
- Training auf News und 50 Prozent von Wiki, Evaluation mit News
  -> ?
- Training auf übersetztem CNN/ Dailymail, Evaluation mit News
  -> ?
  
\begin{table}[htb]
\centering
\begin{tabular}{ | p{2.5cm} | p{2.5cm} | p{2.5cm} | p{2.5cm} | }
\hline
\textbf{Modell} & \textbf{R-Recall} & \textbf{R-Precision} & \textbf{R-Measure} \\
\hline
BERT & 15.78 & 10.25 & 12.11 \\
\hline
XLM-R & 0 & 0 & 0 \\
\hline
BART & 0 & 0 & 0 \\
\hline
\end{tabular}
\caption{SOTA-Reproduktion im Modellvergleich.}
\label{table:SotaRougeScores}
\end{table}

% TODO: Experimente erneut in einer Tabelle zusammenfassen, siehe Tabelle im abstraktiven Ansatz, verschiedene Spalten für verschiedene Parameterkonfigurationen anfügen

% TODO: Sliding-Window-Approach bei zu großen Texten beschreiben und entwickeln, beim Laden der Texte, aber als separate Methode, die auf einzelne Texte anwendbar ist, trotzdem mit Map als Batch-Verarbeitung, über Bool in der Config beim Training auswählbar machen, im Beispiel als Methode einbinden, prinzipiell lange Texte behandeln, ggf. über Konkatenation, mindestens im Ausblick herausstellen

% TODO: Kapitel besser strukturieren (format-technisch, inhaltlich), German Abstractive Summarization lesen und einarbeiten, Beschreibung der Datengrundlage nochmal überprüfen, Lorem ipsum und TODO's in beiden Kapiteln herausarbeiten, Hyperparameteroptimierung auf 10-20 Prozent der Daten vornehmen, Verdichtung der Zusammenfassung verdeutlichen, d.h. Token-Reduktion, zudem die prozentuale Kompressionsrate angeben (75% von 512 auf 128 Token), d.h. mit technischen Anpassungen können auch 5 DIN A4-Seiten um x Prozent verdichtet werden (Wie lang ist der Eingangstext? Wie lang ist der Ausgangstext? Wie geht das Modell mit längeren Texten um?)

% TODO: \cite{YAN19} S. 4 rechts, Herausforderung: Encoder overfitted, Decoder underfitted oder andersherum, wird durch HuggingFace-Framework vorgebeugt, \cite{YAN19} S. 5 oben für Evaluation, Vergleichstabelle der Experimente einbinden und beschreiben, typisches Diagramm zur Visualisierung des Trainingsprozesses anfügen, Verhalten des Modells interpretieren und Anpassungen ableiten, bspw. Exploitation wegen der Struktur der Artikel nochmal aufgreifen, ggf. erst bei der sprachtechnischen Adaption, erwähnen, dass dies als Experiment genügt, sprachtechnische Anpassungen dann erst im nächsten Kapitel, Referenzzusammenfassungen mit ROUGE und BLEU bewerten, um Vergleichswerte nennen zu können, Texte manuell zusammenfassen, um ebenfalls einen Vergleichswert von ROUGE und BLEU zu haben\\

% TODO: Dateien vom Taurus herunterladen und synchronisieren
