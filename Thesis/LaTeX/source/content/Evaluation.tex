\chapter{Evaluation}
\thispagestyle{fancy}
\label{chap:Evaluation}

\noindent
In diesem Kapitel werden die Experimente umfangreich evaluiert. Dies geschieht einerseits automatisiert auf Basis der ausgewählten Metriken, andererseits manuell in Form einer qualitativen Analyse der aus den Anhängen A und B generierten Zusammenfassungen.


\section{Automatische Auswertung}
\noindent
In \autoref{table:ExpEvaluation} sind die \ac{ROUGE}-Scores der \ac{TF2TF} aller Experimente gesamtheitlich dokumentiert. Dabei ist die Konfiguration der Experimente jeweils anhand der genutzten \ac{DLR} und der Sprache der eingegangenen Textdaten nachvollziehbar. Die Experimente sind spaltenweise zu lesen. Es gelten die folgenden Abkürzungen: R-[R]ecall, R-[P]recision, R-[M]easure. Die erste Zeile zeigt wertmäßig den \ac{SOTA} der \ac{ATS}, unabhängig von den in dieser Arbeit verwendeten Architekturen.

\begin{table}[htb]
\centering
\small
\begin{tabular}{ | l | c c c | c c c | c c c | }
\hline

DLR / Sprache
& \multicolumn{3}{| c |}{EN} 
& \multicolumn{3}{| c |}{DE} 
& \multicolumn{3}{| c |}{EN \& DE} \\
\cline{2-10}

& R & P & M 
& R & P & M
& R & P & M \\
\hline

state-of-the-art
& 15.96 & 10.34 & 12.22
& - & - & -
& - & - & - \\

bert-base-multilingual-cased 
& 15.78 & 10.25 & 12.11 
& 15.93 & 10.52 & 11.97
& 16.76 & 11.18 & 12.68 \\

xlm-roberta-base 
& 16.49 & 17.12 & 16.25 
& 15.93 & 10.32 & 11.86 
& 16.44 & 15.45 & 15.35 \\

facebook/mbart-large-cc25 
& - & - & - 
& - & - & - 
& 15.01 & 12.43 & 13.10 \\
\hline

\end{tabular}
\caption{Ergebnisse im Experimentvergleich.}
\label{table:ExpEvaluation}
\end{table}
\newpage

\noindent
\textbf{Experiment 1: Reproduktion auf englischen Daten}\\
\noindent
Im ersten Experiment geschah die Reproduktion des \ac{SOTA} auf Basis englischsprachiger Textdaten. Das \ac{TF2TF} ist demnach unter Nutzung von \ac{BERT} und \ac{XLM-R} weitestgehend \ac{SOTA}-konform. Im Bereich der \ac{XLM-R} ist jedoch ein ungewöhnlich hoher R-Precision-Score auffallend, welcher sich in der Berechnung auch auf den R-Measure-Score auswirkt. Dies bedeutet per Formel eine hohe Übereinstimmung zwischen den Wörtern der Originaltexte und der generierten Zusammenfassungen. Eine weitergehende Ursache hierfür wird im nachfolgenden Kapitel ergründet. Die Lernfortschritte sind anhand der Fehlerentwicklung in \autoref{pic:ExpEnLoss} visualisiert. Hierbei ist zu erkennen, dass \ac{XLM-R} erst spät, dann aber ebenbürtig lernt.\\

\begin{figure}[h]
  \centering
  \fbox{\includegraphics[width=0.55\linewidth]{./source/images/lossenglish.png}}
  \caption{Loss in der SOTA-Reproduktion im Modellvergleich.}
  \label{pic:ExpEnLoss}
\end{figure}

\noindent
\textbf{Experiment 2: Adaption auf deutschen Daten}\\
\noindent
Im zweiten Experiment geschah erstmalig die sprachtechnische Adaption der \ac{TF2TF}. Hierfür kamen demnach ausschließlich deutschsprachige Daten zum Einsatz. \autoref{table:ExpEvaluation} gibt hierbei zu erkennen, dass das Modell unter Nutzung von \ac{BERT} qualitativ in Englisch genauso gut wie in Deutsch ist. Im Bereich der \ac{XLM-R} sind nun keine auffallenden Werte mehr zu verzeichnen. Wertmäßig gleichen sich die \ac{TF2TF} nun unter Nutzung beider \ac{DLR} unter Akzeptanz eines gewissen Rauschens auf \ac{SOTA}-Niveau. Die Lernfortschritte sind in bekannter Weise in \autoref{pic:ExpDeLoss} visualisiert.
\newpage

\begin{figure}[h]
  \centering
  \fbox{\includegraphics[width=0.55\linewidth]{./source/images/lossgerman.png}}
  \caption{Loss in der Adaption auf deutschen Daten im Modellvergleich.}
  \label{pic:ExpDeLoss}
\end{figure}

\noindent
\textbf{Experiment 3: Adaption auf multilingualen Daten}\\
\noindent
Im dritten Experiment geschah die sprachtechnische Adaption der \ac{TF2TF} auf Basis multilingualer Textdaten. \ac{BART} wird indes aus externer Quelle bezogen. Die Evaluation geschieht im Kontext dieser Arbeit ausschließlich auf deutschsprachigen Textdaten. In \autoref{table:ExpEvaluation} wird ersichtlich, dass sich das Modell unter Nutzung von \ac{BERT} nun über den \ac{SOTA} hinaus verbessert, während im Bereich der \ac{XLM-R} erneut auffallende Werte entstehen. \ac{BART} entspricht ebenfalls dem \ac{SOTA}, auch ohne Fine-Tuning. Hier sei weiterhin angemerkt, dass er den \ac{SOTA} auf Basis einer englischsprachigen Evaluation übersteigt. In \autoref{pic:ExpMlLoss} ist abermals die recht späte Konvergenz der \ac{XLM-R} nachvollziehbar, welche womöglich auf die architektonischen Unterschiede der \ac{DLR} zurückzuführen ist.\\

\begin{figure}[h]
  \centering
  \fbox{\includegraphics[width=0.55\linewidth]{./source/images/lossmultilingual.png}}
  \caption{Loss in der Adaption auf multilingualen Daten im Modellvergleich.}
  \label{pic:ExpMlLoss}
\end{figure}
\newpage


\section{Qualitative Analyse}
\noindent
Die soeben wertmäßig evaluierten Experimente werden nun ergänzend qualitativ analysiert. Hierfür werden die \ac{TF2TF} aller Experimente sprachabhängig entweder Anhang A oder Anhang B unterzogen, indem die dort enthaltenen mitunter herausfordernden Texte maschinell zusammengefasst und subjektiv bewertet werden. Die Bewertung geschieht anhand des ursprünglichen Textes und dem angefügten Gold-Standard. Eine derartige Analyse ist erforderlich, um die Qualität sowie die praktische Eignung der trainierten \ac{TF2TF} vollends beurteilen zu können. Die qualitative Analyse ist außerdem erforderlich, um den teilweise kritisch begutachteten sowie statistisch veranlagten \ac{ROUGE}-Score qualitativ einzuordnen. Die in diesem Kapitel referenzierten Zusammenfassungen sind in englischer und deutscher Sprache modellübergreifend stets den Anhängen A und B zu entnehmen.\\

\noindent
\textbf{Experiment 1: Reproduktion auf englischen Daten}\\
\noindent
Ergänzend zum ersten Experiment ist nun zu analysieren, ob das \ac{TF2TF} unter Nutzung von \ac{BERT} tatsächlich \ac{SOTA}-konform ist und wie die auffallenden Werte unter Nutzung von \ac{XLM-R} zu erklären sind.\\

\noindent
\ac{BERT} scheint die beispielhaften Texte sehr gut verstehen zu können. Dies war zu erwarten, da \ac{BERT} bekanntermaßen hervorragend als Encoder funktioniert, etwas beschwerlicher hingegen als Decoder. Dies ist jedoch bis auf eine Ausnahme im Text über das Finale der WM 2014 nicht herauszulesen. Dabei sind die entsprechenden Zusammenfassungen nicht zur orthographisch und grammatikalisch korrekt, sondern weitestgehend auch mit den wichtigsten Informationen ausgestattet. Das \ac{TF2TF} unter Nutzung von \ac{BERT} gilt daher tatsächlich als \ac{SOTA}-konform.\\

\noindent
\ac{XLM-R} verspricht eine wertmäßige Verbesserung. In den generierten Zusammenfassungen wird die inhaltliche Unzulänglichkeit jedoch für den menschlichen Leser auch ohne Kenntnis über die ursprünglichen Texte sehr schnell deutlich. Gleichermaßen ist hier nachvollziehbar, wie die hohen Werte im Bereich der R-Precision und der R-Measure entstanden sind: Die als wichtig erkannten Worte oder Wortpassagen der ursprünglichen Texte werden in den Zusammenfassungen sehr häufig wiederholt. Diese Form der Exploitation hat im Kontext der \ac{ATS} einen höchst nachteiligen Effekt, da inhaltlich keine qualitativen Zusammenfassungen entstehen. Die gegenüber \ac{BERT} komplexere Architektur der \ac{XLM-R} scheint mit den zugeführten Textdaten nicht korrekt lernen zu können und gilt hiermit nicht mehr als \ac{SOTA}-konform.
\newpage

\noindent
\textbf{Experiment 2: Adaption auf deutschen Daten}\\
\noindent
Ergänzend zum zweiten Experiment ist zu analysieren, ob die Adaption der \ac{TF2TF} unter Nutzung von \ac{BERT} und \ac{XLM-R} auf deutschen Daten qualitativ ist. \ac{BERT} scheint die beispielhaften Texte trotz gleichbleibender Werte mit vermehrt fehlerhaften sowie teils widersprüchlichen Informationen zusammenzufassen. Dies ist auch gleichbleibend, wenn \ac{BERT} durch \ac{GBERT} ausgetauscht wird. \ac{XLM-R} weist trotz normalisierter Werte auf \ac{SOTA}-Niveau ein ähnlich fehlerbehaftetes Verhalten auf.\\

\noindent
Hierbei wird einerseits die Abhängigkeit der Modelle vom Umfang der Textdaten deutlich. Dies betrifft sowohl das Vortraining der verwendeten \ac{DLR} als auch das Fine-Tuning der \ac{TF2TF}. Dabei stehen in englischer Sprache so viele Textdaten wie in keiner anderen Sprache frei zur Verfügung. Folglich ist es nicht verwunderlich, wenn die verborgenen Strukturen anderer Sprachen nicht vollends erlernt werden können und die Qualität dadurch limitiert wird.\\

\noindent
Andererseits wird die Abhängigkeit der Modelle von der Beschaffenheit der Textdaten deutlich, insbesondere anhand der Natur der Zusammenfassungen. Hier ist nämlich ein typisches Element der Nachrichtendomäne erkennbar, da anstatt einer konventionellen Zusammenfassung oftmals ein Teaser generiert wird, der den Leser binden und zum eigentlichen Text weiterleiten soll. Dieses Verhalten ist jedoch nicht auf das Modell zurückzuführen, sondern auf die Trainingsdaten, welche größtenteils eben jener Nachrichtendomäne entstammen. Dies unterstreicht insgesamt die Notwendigkeit einer intensiven Datenanalyse. Hinsichtlich eines praktischen Einsatzes ist also stets ein anforderungsgerechter Korpus zu formieren. Zudem ist der Einfluss des vortrainierten Modells bemerkbar, beispielsweise im Text über das Finale der WM 2014. Hier wird im originalen Text von Jogi Löw geschrieben, in der Zusammenfassung hingegen von Jürgen Klopp. Letzterer wurde zuvor jedoch nie erwähnt. Dies kann mit der geringen Distanz beider Personen im Vektorraum erklärt werden und ist somit zwar sachlogisch falsch, mathematisch jedoch nachvollziehbar. Dies tritt in ähnlicher Weise auch in den anderen Zusammenfassungen auf. Ein Fine-Tuning mit sehr viel mehr Trainingsdaten verspricht dieses Rauschen hinreichend zu minimieren.
\newpage

\noindent
\textbf{Experiment 3: Adaption auf multilingualen Daten}\\
\noindent
Ergänzend zum dritten Experiment ist zu analysieren, inwiefern wie wertmäßigen Fortschritte auch in den generierten Zusammenfassungen erkennbar sind. Außerdem ist erstmalig die Qualität von \ac{BART} zu untersuchen.\\

\noindent
Das \ac{TF2TF} unter Nutzung von \ac{BERT} verbessert sich in Englisch nun bis hin zur praktischen Eignung, in Deutsch jedoch nicht. Das \ac{TF2TF} unter Nutzung von \ac{XLM-R} erzielt hingegen keine Verbesserung. Dies ist vermutlich auf die erneut ungewöhnlich hohen Werte im Bereich der R-Precision und der R-Measure zurückzuführen. \ac{XLM-R} scheint große Probleme mit den vorliegenden englischsprachigen Textdaten zu haben.\\

\noindent
\ac{BART} weist aufgrund der identischen Datengrundlage ein ähnliches Verhalten wie die monolingualen \ac{TF2TF} auf, scheint jedoch insgesamt robuster zu sein und auch ohne Fine-Tuning weniger Fehler zu machen. Dennoch sind auch hier inhaltliche Mängel sowie erstmalig auch englische Wörter in deutschen Zusammenfassungen zu verzeichnen. Diese können potenziell durch ein entsprechendes Fine-Tuning ausgemerzt werden. Entgegen obig formulierter Erwartung scheint \ac{BART} dem lockenden Element der Nachrichtendomäne nicht nachzukommen. \ac{BART} ist für die \ac{ATS} in englischer Sprache das beste Modell, in deutscher Sprache vermutlich nur mit einem hinreichenden Fine-Tuning. \ac{BERT} ist in englischer Sprache wertmäßig und subjektiv wider Erwarten nur leicht unterlegen, in deutscher Sprache hingegen deutlich.\\

\noindent
\textbf{Evaluation der Gold-Standards}\\
\noindent
Vergleicht man die Gold-Standards mit den maschinell generierten Zusammenfassungen, so werden die oben beschriebenen Eindrücke bestätigt. \autoref{table:GoldScores} vergleicht die \ac{ROUGE}-Scores aller multilingual vortrainierten Modelle auf Basis eben dieser Gold-Standards. Die Tabelle ist in bekannter Weise zu interpretieren, wobei die Sprache nun die Sprache der Zusammenfassungen beschreibt.

\begin{table}[htb]
\centering
\small
\begin{tabular}{ | l | c c c | c c c | c c c | }
\hline

DLR / Sprache
& \multicolumn{3}{| c |}{EN} 
& \multicolumn{3}{| c |}{DE} \\
\cline{2-7}

& R & P & M 
& R & P & M \\
\hline

state-of-the-art
& 15.96 & 10.34 & 12.22
& - & - & - \\

bert-base-multilingual-cased 
& 24.41 & 25.12 & 24.58
& 6.98 & 10.74 & 8.41 \\

xlm-roberta-base 
& 18.35 & 19.19 & 18.55
& 10.20 & 12.84 & 11.16 \\

facebook/mbart-large-cc25 
& 26.12 & 20.54 & 22.91
& 11.49 & 16.01 & 12.94 \\
\hline

\end{tabular}
\caption{Ergebnisse des Vergleichs mit dem Gold-Standard.}
\label{table:GoldScores}
\end{table}
\newpage

\noindent
Inwieweit diese Werte belastbar sind, ist außerhalb dieser Arbeit zu ergründen. Es lässt sich nur mutmaßen, warum die Werte derart unterschiedlich ausfallen: Menschen können beim Verfassen der Gold-Standards beispielsweise Kontextwissen nutzen, welches gar nicht im ursprünglichen Text stand. Zudem besteht die Hypothese, dass Menschen unterbewusst N-Gramme aus dem ursprünglichen Text übernehmen, wodurch die Werte ungewöhnlich hoch sind. Abschließend ließe sich beurteilen, ob die in \autoref{table:GoldScores} dokumentierten Werte als obere Grenze interpretiert werden können.
