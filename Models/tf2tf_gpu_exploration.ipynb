{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf2tf_gpu_exploration.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fDrVbZL7HVW"
      },
      "source": [
        "# Installation\n",
        "%%capture\n",
        "\n",
        "!pip install transformers==4.5.1\n",
        "!pip install datasets==1.6.2\n",
        "!pip install tokenizers==0.10.2\n",
        "!pip install torch==1.8.1+cu111\n",
        "!pip install psutil==5.8.0\n",
        "!pip install rouge_score\n",
        "!pip install sacrebleu\n",
        "!pip install openpyxl\n",
        "!pip install xlrd\n",
        "!pip install git-python\n",
        "!pip install -U ipython==7.20\n",
        "!pip install cmake\n",
        "!pip install SentencePiece"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaMmJLP17NWS"
      },
      "source": [
        "# Imports\n",
        "import csv\n",
        "import datasets\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "import psutil\n",
        "import pandas as pd\n",
        "import string\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "from collections import Counter\n",
        "from datasets import ClassLabel\n",
        "from nltk import ngrams\n",
        "from IPython.display import display, HTML\n",
        "from typing import List, Tuple"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4K--9R9x7Oht",
        "outputId": "b36a9545-2823-4db8-f9d6-fb0b9c4210d3"
      },
      "source": [
        "# Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "path_drive = \"/content/drive/My Drive/Temp/\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eolHDOUn7QC2",
        "outputId": "3c5f8a99-9d5b-4978-c7ec-e864431bb027"
      },
      "source": [
        "# Config\n",
        "language: str = \"german\"  # english, german, multilingual\n",
        "model_name: str = \"bert-base-multilingual-cased\"\n",
        "tokenizer_name: str = \"bert-base-multilingual-cased\"\n",
        "batch_size: int = 8\n",
        "\n",
        "ratio_corpus_wik: float = 0.25\n",
        "ratio_corpus_nws: float = 0.25\n",
        "ratio_corpus_mls: float = 0.25\n",
        "ratio_corpus_eng: float = 0.25\n",
        "\n",
        "path_output: str = \"/content/drive/My Drive/Temp/Models/\"\n",
        "path_checkpoint: str = \"/content/drive/My Drive/Temp/Models/DE/BERT\"\n",
        "\n",
        "train_size: float = 0.900\n",
        "val_size: float = 0.025\n",
        "test_size: float = 0.075\n",
        "\n",
        "'''\n",
        "- bert-base-multilingual-cased\n",
        "- deepset/gbert-base\n",
        "- xlm-roberta-base\n",
        "- facebook/mbart-large-cc25\n",
        "'''"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n- bert-base-multilingual-cased\\n- deepset/gbert-base\\n- xlm-roberta-base\\n- facebook/mbart-large-cc25\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QunBJaY979CA"
      },
      "source": [
        "# Helpers\n",
        "def load_data() -> Tuple[datasets.Dataset, datasets.Dataset, datasets.Dataset]:\n",
        "    if language == \"english\":\n",
        "        return load_english_data()\n",
        "\n",
        "    if language == \"german\":\n",
        "        return load_german_data()\n",
        "\n",
        "    if language == \"multilingual\":\n",
        "        return load_multilingual_data()\n",
        "\n",
        "\n",
        "def load_english_data() -> Tuple[datasets.Dataset, datasets.Dataset, datasets.Dataset]:\n",
        "    train_data = datasets.load_dataset(\n",
        "        \"cnn_dailymail\", \"3.0.0\",\n",
        "        split=\"train\",\n",
        "        ignore_verifications=True\n",
        "    )\n",
        "\n",
        "    val_data = datasets.load_dataset(\n",
        "        \"cnn_dailymail\", \"3.0.0\",\n",
        "        split=\"validation[:50%]\",\n",
        "        ignore_verifications=True\n",
        "    )\n",
        "\n",
        "    test_data = datasets.load_dataset(\n",
        "        \"cnn_dailymail\", \"3.0.0\",\n",
        "        split=\"test[:50%]\",\n",
        "        ignore_verifications=True\n",
        "    )\n",
        "\n",
        "    train_data = train_data.select(\n",
        "        range(0, int(len(train_data) * ratio_corpus_eng))\n",
        "    )\n",
        "\n",
        "    train_data = train_data.rename_column(\"article\", \"text\")\n",
        "    train_data = train_data.rename_column(\"highlights\", \"summary\")\n",
        "    train_data = train_data.remove_columns(\"id\")\n",
        "\n",
        "    val_data = val_data.rename_column(\"article\", \"text\")\n",
        "    val_data = val_data.rename_column(\"highlights\", \"summary\")\n",
        "    val_data = val_data.remove_columns(\"id\")\n",
        "\n",
        "    test_data = test_data.rename_column(\"article\", \"text\")\n",
        "    test_data = test_data.rename_column(\"highlights\", \"summary\")\n",
        "    test_data = test_data.remove_columns(\"id\")\n",
        "\n",
        "    return train_data.shuffle(), val_data.shuffle(), test_data.shuffle()\n",
        "\n",
        "\n",
        "def load_german_data() -> Tuple[datasets.Dataset, datasets.Dataset, datasets.Dataset]:\n",
        "    ds_wik = load_corpus_wik()\n",
        "    ds_nws = load_corpus_nws()\n",
        "    ds_mls = load_corpus_mls()\n",
        "\n",
        "    german_data = datasets.concatenate_datasets([\n",
        "        ds_wik.select(\n",
        "            range(0, int(len(ds_wik) * ratio_corpus_wik))),\n",
        "        ds_nws.select(\n",
        "            range(0, int(len(ds_nws) * ratio_corpus_nws))),\n",
        "        ds_mls.select(\n",
        "            range(0, int(len(ds_mls) * ratio_corpus_mls)))\n",
        "    ])\n",
        "\n",
        "    train_size = int(len(german_data) * 0.900)\n",
        "    valid_size = int(len(german_data) * 0.025)\n",
        "    test_size = int(len(german_data) * 0.075)\n",
        "\n",
        "    train_data = german_data.select(\n",
        "        range(0, train_size)\n",
        "    )\n",
        "\n",
        "    val_data = german_data.select(\n",
        "        range(train_size, train_size + valid_size)\n",
        "    )\n",
        "\n",
        "    test_data = german_data.select(\n",
        "        range(train_size + valid_size, train_size + valid_size + test_size)\n",
        "    )\n",
        "\n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "\n",
        "def load_corpus_wik() -> datasets.Dataset:\n",
        "    data_txt, data_ref = [], []\n",
        "\n",
        "    with open(\"/content/drive/My Drive/Temp/Corpus/data_train.csv\", \"r\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.reader(f, delimiter=\",\", quoting=csv.QUOTE_ALL)\n",
        "        next(reader, None)\n",
        "\n",
        "        for row in reader:\n",
        "            data_txt.append(row[0])\n",
        "            data_ref.append(row[1])\n",
        "\n",
        "    df_wik = pd.DataFrame(\n",
        "        list(zip(data_txt, data_ref)),\n",
        "        columns=[\"text\", \"summary\"]\n",
        "    )\n",
        "\n",
        "    ds_wik = datasets.arrow_dataset.Dataset.from_pandas(df_wik)\n",
        "\n",
        "    return ds_wik.shuffle()\n",
        "\n",
        "\n",
        "def load_corpus_nws() -> datasets.Dataset:\n",
        "    df_nws = pd.read_excel(\"/content/drive/My Drive/Temp/Corpus/data_train_test.xlsx\", engine=\"openpyxl\")\n",
        "    df_nws = df_nws[[\"article\", \"highlights\"]]\n",
        "    df_nws.columns = [\"text\", \"summary\"]\n",
        "    df_nws = df_nws[~df_nws[\"summary\"].str.contains(\"ZEIT\")]\n",
        "    df_nws = df_nws.dropna()\n",
        "    ds_nws = datasets.arrow_dataset.Dataset.from_pandas(df_nws)\n",
        "    ds_nws = ds_nws.remove_columns(\"__index_level_0__\")\n",
        "\n",
        "    return ds_nws.shuffle()\n",
        "\n",
        "\n",
        "def load_corpus_mls() -> datasets.Dataset:\n",
        "    ds_mls = datasets.load_dataset(\"mlsum\", \"de\", split=\"train\")\n",
        "    ds_mls = ds_mls.remove_columns([\"topic\", \"url\", \"title\", \"date\"])\n",
        "\n",
        "    text_corpus_mls = []\n",
        "    summary_corpus_mls = []\n",
        "\n",
        "    for entry in ds_mls:\n",
        "        text = entry[\"text\"]\n",
        "        summary = entry[\"summary\"]\n",
        "\n",
        "        if summary in text:\n",
        "            text = text[len(summary) + 1:len(text)]\n",
        "\n",
        "        text_corpus_mls.append(text)\n",
        "        summary_corpus_mls.append(summary)\n",
        "\n",
        "    df_mls = pd.DataFrame(\n",
        "        list(zip(text_corpus_mls, summary_corpus_mls)),\n",
        "        columns=[\"text\", \"summary\"]\n",
        "    )\n",
        "\n",
        "    ds_mls = datasets.arrow_dataset.Dataset.from_pandas(df_mls)\n",
        "\n",
        "    return ds_mls.shuffle()\n",
        "\n",
        "\n",
        "def load_multilingual_data() -> Tuple[datasets.Dataset, datasets.Dataset, datasets.Dataset]:\n",
        "    english_data, _, _ = load_english_data()\n",
        "    german_data, _, _ = load_german_data()\n",
        "\n",
        "    multilingual_data = datasets.concatenate_datasets([\n",
        "        german_data, english_data\n",
        "    ]).shuffle()\n",
        "\n",
        "    train_size = int(len(multilingual_data) * 0.900)\n",
        "    valid_size = int(len(multilingual_data) * 0.025)\n",
        "    test_size = int(len(multilingual_data) * 0.075)\n",
        "\n",
        "    train_data = multilingual_data.select(\n",
        "        range(0, train_size)\n",
        "    )\n",
        "\n",
        "    val_data = multilingual_data.select(\n",
        "        range(train_size, train_size + valid_size)\n",
        "    )\n",
        "\n",
        "    test_data = multilingual_data.select(\n",
        "        range(train_size + valid_size, train_size + valid_size + test_size)\n",
        "    )\n",
        "\n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "\n",
        "def test_cuda() -> None:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"Device:\", device)\n",
        "    print(\"Version:\", torch.__version__)\n",
        "\n",
        "\n",
        "def explore_corpus(data: datasets.Dataset) -> None:\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    text_list = []\n",
        "    summary_list = []\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        text = row[\"text\"]\n",
        "        summary = row[\"summary\"]\n",
        "        text_list.append(len(text))\n",
        "        summary_list.append(len(summary))\n",
        "\n",
        "    df = pd.DataFrame(data[:1])\n",
        "\n",
        "    for column, typ in data.features.items():\n",
        "        if isinstance(typ, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "\n",
        "\n",
        "def empty_cache() -> None:\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    psutil.virtual_memory()\n",
        "\n",
        "\n",
        "def load_tokenizer_and_model(from_checkpoint: bool = False) -> Tuple[transformers.AutoTokenizer, transformers.EncoderDecoderModel]:\n",
        "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "        tokenizer_name, strip_accent=False\n",
        "    )\n",
        "\n",
        "    if from_checkpoint:\n",
        "        if \"mbart\" in model_name:\n",
        "            tf2tf = transformers.AutoModelForSeq2SeqLM.from_pretrained(\n",
        "                path_checkpoint\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            tf2tf = transformers.EncoderDecoderModel.from_pretrained(\n",
        "                path_checkpoint\n",
        "            )\n",
        "\n",
        "    else:\n",
        "        if \"mbart\" in model_name:\n",
        "            tf2tf = transformers.MBartForConditionalGeneration.from_pretrained(\n",
        "                model_name\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            tf2tf = transformers.EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
        "                model_name, model_name, tie_encoder_decoder=True\n",
        "            )\n",
        "\n",
        "    return tokenizer, tf2tf\n",
        "\n",
        "\n",
        "def configure_model(tf2tf: transformers.EncoderDecoderModel, tokenizer: transformers.AutoTokenizer):\n",
        "    tf2tf.config.decoder_start_token_id = tokenizer.cls_token_id\n",
        "    tf2tf.config.bos_token_id = tokenizer.bos_token_id\n",
        "    tf2tf.config.eos_token_id = tokenizer.sep_token_id\n",
        "    tf2tf.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    tf2tf.config.max_length = 128\n",
        "    tf2tf.config.min_length = 56\n",
        "    tf2tf.config.no_repeat_ngram_size = 3\n",
        "    tf2tf.config.early_stopping = True\n",
        "    tf2tf.config.length_penalty = 2.0\n",
        "    tf2tf.config.num_beams = 2\n",
        "\n",
        "    return tf2tf\n",
        "\n",
        "\n",
        "def split_long_texts(parts: List[str], text: str):\n",
        "    limit = 512\n",
        "\n",
        "    if len(text) > limit:\n",
        "        end_index = max([\n",
        "            text.rfind(\".\", 0, limit),\n",
        "            text.rfind(\"!\", 0, limit),\n",
        "            text.rfind(\"?\", 0, limit)\n",
        "        ])\n",
        "\n",
        "        parts.append(text[0:end_index + 1].strip())\n",
        "        text = text[end_index + 1:len(text)].strip()\n",
        "        parts = split_long_texts(parts, text)\n",
        "\n",
        "    else:\n",
        "        parts.append(text)\n",
        "\n",
        "    return parts"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GK0Axi6U8O-w"
      },
      "source": [
        "# Methods\n",
        "def clean(text: str) -> str:\n",
        "    text = \" \".join([w.lower() for w in text.split()])\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def analyze_text_lenghts(corpus: List[str]) -> None:\n",
        "    lengths = []\n",
        "\n",
        "    for text in corpus:\n",
        "        lengths.append(len(text.split()))\n",
        "\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 6))\n",
        "\n",
        "    plt.hist(lengths, bins=40)\n",
        "    plt.xlabel(\"Anzahl der Wörter\", fontsize=18)\n",
        "    plt.ylabel(\"Anzahl der Texte\", fontsize=18)\n",
        "    plt.xticks(rotation=0, fontsize=12)\n",
        "    plt.yticks(rotation=0, fontsize=12)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    print(sum(lengths) / len(lengths))\n",
        "\n",
        "\n",
        "def analyze_most_common_words(corpus: List[str], k: int = 20) -> None:\n",
        "    words = []\n",
        "\n",
        "    for text in corpus:\n",
        "        token_list = clean(text).split()\n",
        "\n",
        "        for token in token_list:\n",
        "            if len(token) > 1:\n",
        "                words.append(token)\n",
        "\n",
        "    most_common_words = Counter(words).most_common(k)\n",
        "\n",
        "    x = [tuple[0] for tuple in most_common_words]\n",
        "    y = [tuple[1] for tuple in most_common_words]\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 6))\n",
        "\n",
        "    plt.bar(x, y)\n",
        "    plt.xlabel(\"Wörter\", fontsize=18)\n",
        "    plt.ylabel(\"Anzahl\", fontsize=18)\n",
        "    plt.xticks(rotation=90, fontsize=12)\n",
        "    plt.yticks(rotation=0, fontsize=12)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def analyze_n_grams(corpus: List[str], n: int = 3, n_most_common_n_grams: int = 20) -> None:\n",
        "    n_grams = []\n",
        "\n",
        "    for text in corpus:\n",
        "        try:\n",
        "            for n_gram in ngrams(text.split(), n):\n",
        "                n_grams.append(n_gram)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "\n",
        "    most_common_n_grams = Counter(n_grams).most_common(n_most_common_n_grams)\n",
        "\n",
        "    x = [\" \".join(tuple[0]) for tuple in most_common_n_grams]\n",
        "    y = [tuple[1] for tuple in most_common_n_grams]\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 6))\n",
        "\n",
        "    plt.bar(x, y)\n",
        "    plt.xlabel(\"N-Gramme\", fontsize=18)\n",
        "    plt.ylabel(\"Anzahl\", fontsize=18)\n",
        "    plt.xticks(rotation=90, fontsize=12)\n",
        "    plt.yticks(rotation=0, fontsize=12)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpuxvYBL79jd",
        "outputId": "ef2cc829-3dbb-4fa1-9d65-d15520f9702e"
      },
      "source": [
        "# Exploration\n",
        "data, _, _ = load_data()\n",
        "corpus = list(data[\"text\"])\n",
        "\n",
        "analyze_text_lenghts(corpus)\n",
        "analyze_most_common_words(corpus)\n",
        "analyze_n_grams(corpus, n=2)\n",
        "analyze_n_grams(corpus, n=3)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reusing dataset mlsum (/root/.cache/huggingface/datasets/mlsum/de/1.0.0/77f23eb185781f439927ac2569ab1da1083195d8b2dab2b2f6bbe52feb600688)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}