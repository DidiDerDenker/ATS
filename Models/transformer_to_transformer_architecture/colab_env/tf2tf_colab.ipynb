{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "tf2tf_tud_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0934304c25454ef9988fa62df4680ebd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3b3338d9d9804ef4b5f16ee86275487f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_16a24aa069ee4c76bfc2728421e6e27e",
              "IPY_MODEL_cae193aab3224199b8c8e606cf653878"
            ]
          }
        },
        "3b3338d9d9804ef4b5f16ee86275487f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "16a24aa069ee4c76bfc2728421e6e27e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_212bad6e261346f0ba4b2320d9b0348c",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2444517405,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2444517405,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_908eaa7e54b244cc84cdd315e56e758d"
          }
        },
        "cae193aab3224199b8c8e606cf653878": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c0c74692313d487e9615d6ba41c5c255",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2.44G/2.44G [01:28&lt;00:00, 27.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_545f435e41e8446a8336253d10203c7b"
          }
        },
        "212bad6e261346f0ba4b2320d9b0348c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "908eaa7e54b244cc84cdd315e56e758d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c0c74692313d487e9615d6ba41c5c255": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "545f435e41e8446a8336253d10203c7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5ecab3d6c8dc4981907162258a7256a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_22212bb2b45d44a883af197e3637830a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_226a2de4051a4c63b225e1638c23041e",
              "IPY_MODEL_c1e4e37e76c5435fa65abef0bf88a868"
            ]
          }
        },
        "22212bb2b45d44a883af197e3637830a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "226a2de4051a4c63b225e1638c23041e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1d13ca4206d340aa8d0910b196c56e72",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 5058,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 5058,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cad61df349d0437c913aae2789783626"
          }
        },
        "c1e4e37e76c5435fa65abef0bf88a868": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2f77c646054f4121bbfc11fb06ff3004",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5058/5058 [04:16&lt;00:00, 19.75ba/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_db9ae6c6a89a423d98971e7a8c33d40f"
          }
        },
        "1d13ca4206d340aa8d0910b196c56e72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cad61df349d0437c913aae2789783626": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2f77c646054f4121bbfc11fb06ff3004": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "db9ae6c6a89a423d98971e7a8c33d40f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f010c3af2904499891e0ff72218d387f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_095ee958e2d4435193bc740789f25fe6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_35d1be41efaa44fd84e1488f96e86b94",
              "IPY_MODEL_0c3a1310ce7546dd95802a152a768044"
            ]
          }
        },
        "095ee958e2d4435193bc740789f25fe6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "35d1be41efaa44fd84e1488f96e86b94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b48f3f60e74a43f090d60457db22e3ea",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6a8d63ff586c4475a674f82e2ed42102"
          }
        },
        "0c3a1310ce7546dd95802a152a768044": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_39cdef0b12874be5a4e8e74fc41c0258",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [00:12&lt;00:00, 12.65s/ba]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_acdf993b2f5943dc948cfd4e2075f44c"
          }
        },
        "b48f3f60e74a43f090d60457db22e3ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6a8d63ff586c4475a674f82e2ed42102": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "39cdef0b12874be5a4e8e74fc41c0258": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "acdf993b2f5943dc948cfd4e2075f44c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fuoPV9Z6KlU"
      },
      "source": [
        "# Installation\n",
        "%%capture\n",
        "\n",
        "!pip install transformers==4.5.1\n",
        "!pip install datasets==1.6.2\n",
        "!pip install tokenizers==0.10.2\n",
        "!pip install torch==1.8.1+cu111\n",
        "!pip install psutil==5.8.0\n",
        "!pip install rouge_score\n",
        "!pip install sacrebleu\n",
        "!pip install openpyxl\n",
        "!pip install xlrd\n",
        "!pip install git-python\n",
        "!pip install -U ipython==7.20\n",
        "!pip install cmake\n",
        "!pip install SentencePiece"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNqQkQAh22al"
      },
      "source": [
        "# Imports\n",
        "import gc\n",
        "import csv\n",
        "import torch\n",
        "import psutil\n",
        "import datasets\n",
        "import transformers\n",
        "import pandas as pd\n",
        "\n",
        "from datasets import ClassLabel\n",
        "from IPython.display import display, HTML"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1OYzIUc5Yby",
        "outputId": "b8f0ff1a-4534-4aca-8d8d-53391de898de"
      },
      "source": [
        "# Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "path_drive = \"/content/drive/My Drive/Temp/\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_5QLKCO2nHV"
      },
      "source": [
        "# Config\n",
        "language = \"german\"  # english\n",
        "model_name = \"bert-base-multilingual-cased\"\n",
        "tokenizer_name = \"bert-base-german-cased\"\n",
        "batch_size = 2  # 16\n",
        "\n",
        "ratio_corpus_wiki = 0.1  # 1.0\n",
        "ratio_corpus_news = 0.1  # 1.0\n",
        "\n",
        "path_output = \"./\"\n",
        "path_checkpoint = path_drive + \"Models\"\n",
        "\n",
        "text_english = \"\"\"Almost as soon as the World Trade Center's Twin Towers fell on September 11, 2001, thousands of firefighters, police officers, construction workers, search-and-rescue dogs and volunteers headed to Ground Zero to look for survivors. Because they didnâ€™t know how many people were trapped alive in the wreckage, firefighters and other rescue workers had to search carefully through the unstable piles of rubble for air pockets, called \\\"voids\\\", where they might find people who had been unable to escape from the collapsing buildings. To be safe, they didnâ€™t use any heavy equipment at first. Some dug with their bare hands, while others formed bucket brigades to move small amounts of debris as efficiently as possible. Unfortunately, there were not many survivors to find: Two firemen were pulled from their truck in a cavity beneath some wreckage, and a few people were pinned at the edges of the pile. By September 12, workers had rescued all of the people who were trapped at the site. After that, the Ground Zero workers had a new and more heartbreaking mission: to sift carefully through the debris in search of human remains. The fallen buildings were unstable, and engineers worried that the weight of trucks and cranes would cause the wreckage to shift and collapse again, so the workers had to keep using the bucket brigades. Meanwhile, huge fires continued to burn at the center of the pile. Jagged, sharp pieces of iron and steel were everywhere. The work was so dangerous that many firefighters and police officers wrote their names and phone numbers on their forearms in case they fell into the hole or were crushed.\"\"\"\n",
        "text_german = \"\"\"Der 11. September 2001 war ein schÃ¶ner SpÃ¤tsommertag in New York â€“ bis um 08:46 Uhr ein Flugzeug in den Nordturm des World Trade Centers flog. ZunÃ¤chst ging man von einem tragischen Unfall aus. Dann aber flog eine 2. Boeing in den SÃ¼dturm. Die Bilder, die an diesem Tag und an den folgenden Jahrestagen um die Welt gingen, machen noch heute sprach- und fassungslos. FÃ¼r mehr als 2.500 Menschen wurden die brennenden HochhaustÃ¼rme zur Todesfalle; fast 400 Feuerwehrleute und Polizeibeamte verloren bei den Rettungsarbeiten ihr Leben. Dieser traurige Tag ging fortan als '9/11' in die Geschichte ein. New York stand nach dem Anschlag auf das World Trade Center verstÃ¤ndlicherweise unter Schock und vor einem Desaster unglaublichen AusmaÃŸes. Die TrÃ¼mmer qualmten noch bis in den Dezember 2001 hinein und es sollte rund 9 Monate dauern, bis die insgesamt 1,8 Mio. Tonnen Schutt weggerÃ¤umt waren. Seither klafft an der Stelle, wo zuvor die 'Twin Tower' standen, eine riesige Wunde in Manhattans Stadtbild. Nach den AufrÃ¤umarbeiten auf dem World Trade Center GelÃ¤nde blieb eine riesige Grube zurÃ¼ck: Ground Zero. Am Zaun sind die Ereignisse des 11. September dokumentiert. Seit 2011 gibt es einen Gedenkpavillon, in 2012 wurde das 'National September 11 Memorial and Museum' erÃ¶ffnet.\"\"\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVakMlpm2z_C"
      },
      "source": [
        "# Helpers\n",
        "def load_data(language, ratio_corpus_wiki=0.0, ratio_corpus_news=0.0):\n",
        "    if str(language) == \"english\":\n",
        "        train_data = datasets.load_dataset(\n",
        "            \"cnn_dailymail\", \"3.0.0\", split=\"train\")\n",
        "        val_data = datasets.load_dataset(\n",
        "            \"cnn_dailymail\", \"3.0.0\", split=\"validation[:10%]\")\n",
        "        test_data = datasets.load_dataset(\n",
        "            \"cnn_dailymail\", \"3.0.0\", split=\"test[:5%]\")\n",
        "\n",
        "        return train_data, val_data, test_data\n",
        "\n",
        "    elif str(language) == \"german\":\n",
        "        data_txt, data_ref = [], []\n",
        "\n",
        "        with open(path_drive + \"Corpus/data_train.csv\", \"r\", encoding=\"utf-8\") as f:\n",
        "            reader = csv.reader(f, delimiter=\",\", quoting=csv.QUOTE_ALL)\n",
        "            next(reader, None)\n",
        "\n",
        "            for row in reader:\n",
        "                data_txt.append(row[0])\n",
        "                data_ref.append(row[1])\n",
        "\n",
        "        tuples_wiki = list(zip(data_txt, data_ref))\n",
        "        tuples_wiki = tuples_wiki[0:int(len(tuples_wiki) * ratio_corpus_wiki)]\n",
        "\n",
        "        dataframe = pd.DataFrame(\n",
        "            tuples_wiki, columns=[\"article\", \"highlights\"]\n",
        "        )\n",
        "\n",
        "        tuples_news = pd.read_excel(\n",
        "            path_drive + \"Corpus/data_train_test.xlsx\", engine=\"openpyxl\"\n",
        "        )\n",
        "\n",
        "        tuples_news = tuples_news[0:int(len(tuples_news) * ratio_corpus_news)]\n",
        "        del tuples_news[\"Unnamed: 0\"]\n",
        "\n",
        "        dataframe = pd.concat([dataframe, tuples_news])\n",
        "        dataframe = dataframe.dropna()\n",
        "        dataframe = dataframe[~dataframe[\"highlights\"].str.contains(\"ZEIT\")]\n",
        "\n",
        "        german_data = datasets.arrow_dataset.Dataset.from_pandas(\n",
        "            dataframe[[\"article\", \"highlights\"]]\n",
        "        )\n",
        "\n",
        "        german_data = german_data.shuffle()\n",
        "\n",
        "        train_size = int(len(dataframe) * 0.9)\n",
        "        valid_size = int(len(dataframe) * 0.05)\n",
        "        test_size = int(len(dataframe) * 0.05)\n",
        "\n",
        "        train_data = german_data.select(\n",
        "            range(0, train_size))\n",
        "        val_data = german_data.select(\n",
        "            range(train_size, train_size + valid_size))\n",
        "        test_data = german_data.select(\n",
        "            range(train_size + valid_size, len(dataframe)))\n",
        "        \n",
        "        del german_data\n",
        "\n",
        "        return train_data.shuffle(), val_data.shuffle(), test_data.shuffle()\n",
        "\n",
        "\n",
        "def explore_corpus(data):\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    text_list = []\n",
        "    summary_list = []\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        text = row[\"article\"]\n",
        "        summary = row[\"highlights\"]\n",
        "        text_list.append(len(text))\n",
        "        summary_list.append(len(summary))\n",
        "\n",
        "    df = pd.DataFrame(data[:1])\n",
        "\n",
        "    for column, typ in data.features.items():\n",
        "        if isinstance(typ, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "\n",
        "\n",
        "def test_cuda():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"Device:\", device)\n",
        "    print(\"Version:\", torch.__version__)\n",
        "\n",
        "\n",
        "def empty_cache():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    psutil.virtual_memory()\n",
        "\n",
        "    print(torch.cuda.get_device_properties(0).total_memory)\n",
        "    print(torch.cuda.memory_reserved(0))\n",
        "    print(torch.cuda.memory_allocated(0))\n",
        "\n",
        "    %whos"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0934304c25454ef9988fa62df4680ebd",
            "3b3338d9d9804ef4b5f16ee86275487f",
            "16a24aa069ee4c76bfc2728421e6e27e",
            "cae193aab3224199b8c8e606cf653878",
            "212bad6e261346f0ba4b2320d9b0348c",
            "908eaa7e54b244cc84cdd315e56e758d",
            "c0c74692313d487e9615d6ba41c5c255",
            "545f435e41e8446a8336253d10203c7b",
            "5ecab3d6c8dc4981907162258a7256a4",
            "22212bb2b45d44a883af197e3637830a",
            "226a2de4051a4c63b225e1638c23041e",
            "c1e4e37e76c5435fa65abef0bf88a868",
            "1d13ca4206d340aa8d0910b196c56e72",
            "cad61df349d0437c913aae2789783626",
            "2f77c646054f4121bbfc11fb06ff3004",
            "db9ae6c6a89a423d98971e7a8c33d40f",
            "f010c3af2904499891e0ff72218d387f",
            "095ee958e2d4435193bc740789f25fe6",
            "35d1be41efaa44fd84e1488f96e86b94",
            "0c3a1310ce7546dd95802a152a768044",
            "b48f3f60e74a43f090d60457db22e3ea",
            "6a8d63ff586c4475a674f82e2ed42102",
            "39cdef0b12874be5a4e8e74fc41c0258",
            "acdf993b2f5943dc948cfd4e2075f44c"
          ]
        },
        "id": "2N5LeIgD3Dnb",
        "outputId": "5066f9c8-ed0e-4d6b-d10e-657635afb4cf"
      },
      "source": [
        "# Training\n",
        "tokenizer = transformers.BertTokenizer.from_pretrained(\n",
        "    tokenizer_name\n",
        ")\n",
        "\n",
        "tf2tf = transformers.EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
        "    model_name, model_name, tie_encoder_decoder=False  # True\n",
        ")\n",
        "\n",
        "train_data, val_data, test_data = load_data(\n",
        "    language=language,\n",
        "    ratio_corpus_wiki=ratio_corpus_wiki,\n",
        "    ratio_corpus_news=ratio_corpus_news\n",
        ")\n",
        "\n",
        "explore_corpus(train_data)\n",
        "rouge = datasets.load_metric(\"rouge\")\n",
        "\n",
        "tf2tf.config.decoder_start_token_id = tokenizer.cls_token_id\n",
        "tf2tf.config.bos_token_id = tokenizer.bos_token_id\n",
        "tf2tf.config.eos_token_id = tokenizer.sep_token_id\n",
        "tf2tf.config.pad_token_id = tokenizer.pad_token_id\n",
        "tf2tf.config.vocab_size = tf2tf.config.encoder.vocab_size\n",
        "\n",
        "tf2tf.config.max_length = 142\n",
        "tf2tf.config.min_length = 56\n",
        "tf2tf.config.no_repeat_ngram_size = 3\n",
        "tf2tf.config.early_stopping = True\n",
        "tf2tf.config.length_penalty = 2.0\n",
        "tf2tf.config.num_beams = 4\n",
        "\n",
        "tf2tf.to(\"cuda\")\n",
        "\n",
        "\n",
        "def process_data_to_model_inputs(batch):\n",
        "    encoder_max_length = 512\n",
        "    decoder_max_length = 128\n",
        "\n",
        "    inputs = tokenizer(batch[\"article\"], padding=\"max_length\",\n",
        "                       truncation=True, max_length=encoder_max_length)\n",
        "\n",
        "    outputs = tokenizer(batch[\"highlights\"], padding=\"max_length\",\n",
        "                        truncation=True, max_length=decoder_max_length)\n",
        "\n",
        "    batch[\"input_ids\"] = inputs.input_ids\n",
        "    batch[\"attention_mask\"] = inputs.attention_mask\n",
        "    batch[\"decoder_input_ids\"] = outputs.input_ids\n",
        "    batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
        "    batch[\"labels\"] = outputs.input_ids.copy()\n",
        "    batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels]\n",
        "                       for labels in batch[\"labels\"]]\n",
        "\n",
        "    return batch\n",
        "\n",
        "\n",
        "train_data = train_data.map(\n",
        "    process_data_to_model_inputs,\n",
        "    batched=True,\n",
        "    batch_size=batch_size,\n",
        "    remove_columns=[\"article\", \"highlights\"]\n",
        ")\n",
        "\n",
        "train_data.set_format(\n",
        "    type=\"torch\",\n",
        "    columns=[\"input_ids\",\n",
        "             \"attention_mask\",\n",
        "             \"decoder_input_ids\",\n",
        "             \"decoder_attention_mask\",\n",
        "             \"labels\"]\n",
        ")\n",
        "\n",
        "val_data = val_data.map(\n",
        "    process_data_to_model_inputs,\n",
        "    batched=True,\n",
        "    remove_columns=[\"article\", \"highlights\"]\n",
        ")\n",
        "\n",
        "val_data.set_format(\n",
        "    type=\"torch\",\n",
        "    columns=[\"input_ids\",\n",
        "             \"attention_mask\",\n",
        "             \"decoder_input_ids\",\n",
        "             \"decoder_attention_mask\",\n",
        "             \"labels\"]\n",
        ")\n",
        "\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels_ids = pred.label_ids\n",
        "    pred_ids = pred.predictions\n",
        "\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
        "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "\n",
        "    rouge_output = rouge.compute(\n",
        "        predictions=pred_str,\n",
        "        references=label_str,\n",
        "        rouge_types=[\"rouge2\"]\n",
        "    )[\"rouge2\"].mid\n",
        "\n",
        "    return {\n",
        "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
        "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
        "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
        "    }\n",
        "  \n",
        "\n",
        "test_cuda()\n",
        "empty_cache()\n",
        "steps = 2000\n",
        "\n",
        "training_args = transformers.Seq2SeqTrainingArguments(\n",
        "    predict_with_generate=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    output_dir=path_output,\n",
        "    warmup_steps=1000,\n",
        "    save_steps=steps,\n",
        "    logging_steps=1000,\n",
        "    eval_steps=steps,\n",
        "    eval_accumulation_steps=10,\n",
        "    save_total_limit=1,\n",
        "    fp16=True\n",
        ")\n",
        "\n",
        "trainer = transformers.Seq2SeqTrainer(\n",
        "    model=tf2tf,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=val_data,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0934304c25454ef9988fa62df4680ebd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2444517405.0, style=ProgressStyle(descrâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at facebook/mbart-large-cc25 were not used when initializing MBartForCausalLM: ['final_logits_bias', 'model.shared.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.1.fc1.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.2.fc1.weight', 'model.encoder.layers.2.fc1.bias', 'model.encoder.layers.2.fc2.weight', 'model.encoder.layers.2.fc2.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.3.fc1.weight', 'model.encoder.layers.3.fc1.bias', 'model.encoder.layers.3.fc2.weight', 'model.encoder.layers.3.fc2.bias', 'model.encoder.layers.3.final_layer_norm.weight', 'model.encoder.layers.3.final_layer_norm.bias', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.4.fc1.bias', 'model.encoder.layers.4.fc2.weight', 'model.encoder.layers.4.fc2.bias', 'model.encoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn_layer_norm.bias', 'model.encoder.layers.5.fc1.weight', 'model.encoder.layers.5.fc1.bias', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.5.fc2.bias', 'model.encoder.layers.5.final_layer_norm.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.6.self_attn.k_proj.weight', 'model.encoder.layers.6.self_attn.k_proj.bias', 'model.encoder.layers.6.self_attn.v_proj.weight', 'model.encoder.layers.6.self_attn.v_proj.bias', 'model.encoder.layers.6.self_attn.q_proj.weight', 'model.encoder.layers.6.self_attn.q_proj.bias', 'model.encoder.layers.6.self_attn.out_proj.weight', 'model.encoder.layers.6.self_attn.out_proj.bias', 'model.encoder.layers.6.self_attn_layer_norm.weight', 'model.encoder.layers.6.self_attn_layer_norm.bias', 'model.encoder.layers.6.fc1.weight', 'model.encoder.layers.6.fc1.bias', 'model.encoder.layers.6.fc2.weight', 'model.encoder.layers.6.fc2.bias', 'model.encoder.layers.6.final_layer_norm.weight', 'model.encoder.layers.6.final_layer_norm.bias', 'model.encoder.layers.7.self_attn.k_proj.weight', 'model.encoder.layers.7.self_attn.k_proj.bias', 'model.encoder.layers.7.self_attn.v_proj.weight', 'model.encoder.layers.7.self_attn.v_proj.bias', 'model.encoder.layers.7.self_attn.q_proj.weight', 'model.encoder.layers.7.self_attn.q_proj.bias', 'model.encoder.layers.7.self_attn.out_proj.weight', 'model.encoder.layers.7.self_attn.out_proj.bias', 'model.encoder.layers.7.self_attn_layer_norm.weight', 'model.encoder.layers.7.self_attn_layer_norm.bias', 'model.encoder.layers.7.fc1.weight', 'model.encoder.layers.7.fc1.bias', 'model.encoder.layers.7.fc2.weight', 'model.encoder.layers.7.fc2.bias', 'model.encoder.layers.7.final_layer_norm.weight', 'model.encoder.layers.7.final_layer_norm.bias', 'model.encoder.layers.8.self_attn.k_proj.weight', 'model.encoder.layers.8.self_attn.k_proj.bias', 'model.encoder.layers.8.self_attn.v_proj.weight', 'model.encoder.layers.8.self_attn.v_proj.bias', 'model.encoder.layers.8.self_attn.q_proj.weight', 'model.encoder.layers.8.self_attn.q_proj.bias', 'model.encoder.layers.8.self_attn.out_proj.weight', 'model.encoder.layers.8.self_attn.out_proj.bias', 'model.encoder.layers.8.self_attn_layer_norm.weight', 'model.encoder.layers.8.self_attn_layer_norm.bias', 'model.encoder.layers.8.fc1.weight', 'model.encoder.layers.8.fc1.bias', 'model.encoder.layers.8.fc2.weight', 'model.encoder.layers.8.fc2.bias', 'model.encoder.layers.8.final_layer_norm.weight', 'model.encoder.layers.8.final_layer_norm.bias', 'model.encoder.layers.9.self_attn.k_proj.weight', 'model.encoder.layers.9.self_attn.k_proj.bias', 'model.encoder.layers.9.self_attn.v_proj.weight', 'model.encoder.layers.9.self_attn.v_proj.bias', 'model.encoder.layers.9.self_attn.q_proj.weight', 'model.encoder.layers.9.self_attn.q_proj.bias', 'model.encoder.layers.9.self_attn.out_proj.weight', 'model.encoder.layers.9.self_attn.out_proj.bias', 'model.encoder.layers.9.self_attn_layer_norm.weight', 'model.encoder.layers.9.self_attn_layer_norm.bias', 'model.encoder.layers.9.fc1.weight', 'model.encoder.layers.9.fc1.bias', 'model.encoder.layers.9.fc2.weight', 'model.encoder.layers.9.fc2.bias', 'model.encoder.layers.9.final_layer_norm.weight', 'model.encoder.layers.9.final_layer_norm.bias', 'model.encoder.layers.10.self_attn.k_proj.weight', 'model.encoder.layers.10.self_attn.k_proj.bias', 'model.encoder.layers.10.self_attn.v_proj.weight', 'model.encoder.layers.10.self_attn.v_proj.bias', 'model.encoder.layers.10.self_attn.q_proj.weight', 'model.encoder.layers.10.self_attn.q_proj.bias', 'model.encoder.layers.10.self_attn.out_proj.weight', 'model.encoder.layers.10.self_attn.out_proj.bias', 'model.encoder.layers.10.self_attn_layer_norm.weight', 'model.encoder.layers.10.self_attn_layer_norm.bias', 'model.encoder.layers.10.fc1.weight', 'model.encoder.layers.10.fc1.bias', 'model.encoder.layers.10.fc2.weight', 'model.encoder.layers.10.fc2.bias', 'model.encoder.layers.10.final_layer_norm.weight', 'model.encoder.layers.10.final_layer_norm.bias', 'model.encoder.layers.11.self_attn.k_proj.weight', 'model.encoder.layers.11.self_attn.k_proj.bias', 'model.encoder.layers.11.self_attn.v_proj.weight', 'model.encoder.layers.11.self_attn.v_proj.bias', 'model.encoder.layers.11.self_attn.q_proj.weight', 'model.encoder.layers.11.self_attn.q_proj.bias', 'model.encoder.layers.11.self_attn.out_proj.weight', 'model.encoder.layers.11.self_attn.out_proj.bias', 'model.encoder.layers.11.self_attn_layer_norm.weight', 'model.encoder.layers.11.self_attn_layer_norm.bias', 'model.encoder.layers.11.fc1.weight', 'model.encoder.layers.11.fc1.bias', 'model.encoder.layers.11.fc2.weight', 'model.encoder.layers.11.fc2.bias', 'model.encoder.layers.11.final_layer_norm.weight', 'model.encoder.layers.11.final_layer_norm.bias', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layernorm_embedding.bias', 'model.encoder.layer_norm.weight', 'model.encoder.layer_norm.bias']\n",
            "- This IS expected if you are initializing MBartForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing MBartForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of MBartForCausalLM were not initialized from the model checkpoint at facebook/mbart-large-cc25 and are newly initialized: ['lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ecab3d6c8dc4981907162258a7256a4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=5058.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f010c3af2904499891e0ff72218d387f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Device: cuda\n",
            "Version: 1.8.1+cu101\n",
            "17071734784\n",
            "4303355904\n",
            "4277977088\n",
            "Variable                       Type                   Data/Info\n",
            "---------------------------------------------------------------\n",
            "ClassLabel                     type                   <class 'datasets.features.ClassLabel'>\n",
            "HTML                           type                   <class 'IPython.core.display.HTML'>\n",
            "batch_size                     int                    2\n",
            "compute_metrics                function               <function compute_metrics at 0x7f914e5ff050>\n",
            "csv                            module                 <module 'csv' from '/usr/lib/python3.7/csv.py'>\n",
            "datasets                       module                 <module 'datasets' from '<...>es/datasets/__init__.py'>\n",
            "display                        function               <function display at 0x7f9398293dd0>\n",
            "drive                          module                 <module 'google.colab.dri<...>s/google/colab/drive.py'>\n",
            "empty_cache                    function               <function empty_cache at 0x7f930de9af80>\n",
            "explore_corpus                 function               <function explore_corpus at 0x7f930fb27b90>\n",
            "gc                             module                 <module 'gc' (built-in)>\n",
            "language                       str                    german\n",
            "load_data                      function               <function load_data at 0x7f930fb27c20>\n",
            "model_name                     str                    facebook/mbart-large-cc25\n",
            "path_checkpoint                str                    /content/drive/My Drive/Temp/Models\n",
            "path_drive                     str                    /content/drive/My Drive/Temp/\n",
            "path_output                    str                    ./\n",
            "pd                             module                 <module 'pandas' from '/u<...>ages/pandas/__init__.py'>\n",
            "process_data_to_model_inputs   function               <function process_data_to<...>inputs at 0x7f92dc8f2200>\n",
            "psutil                         module                 <module 'psutil' from '/u<...>ages/psutil/__init__.py'>\n",
            "ratio_corpus_news              float                  0.1\n",
            "ratio_corpus_wiki              float                  0.1\n",
            "rouge                          Rouge                  Metric(name: \"rouge\", fea<...>n\"\"\", stored examples: 0)\n",
            "test_cuda                      function               <function test_cuda at 0x7f930de9aef0>\n",
            "test_data                      Dataset                Dataset({\\n    features: <...>],\\n    num_rows: 563\\n})\n",
            "text_english                   str                    Almost as soon as the Wor<...>the hole or were crushed.\n",
            "text_german                    str                    Der 11. September 2001 wa<...>ial and Museum' erÃ¶ffnet.\n",
            "tf2tf                          EncoderDecoderModel    EncoderDecoderModel(\\n  (<...>0027, bias=False)\\n  )\\n)\n",
            "tokenizer                      BertTokenizer          PreTrainedTokenizer(name_<...> 'mask_token': '[MASK]'})\n",
            "tokenizer_name                 str                    bert-base-german-cased\n",
            "torch                          module                 <module 'torch' from '/us<...>kages/torch/__init__.py'>\n",
            "train_data                     Dataset                Dataset({\\n    features: <...>\\n    num_rows: 10115\\n})\n",
            "transformers                   _LazyModule            <module 'transformers' fr<...>ransformers/__init__.py'>\n",
            "val_data                       Dataset                Dataset({\\n    features: <...>],\\n    num_rows: 561\\n})\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-bca931206374>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    135\u001b[0m )\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, **kwargs)\u001b[0m\n\u001b[1;32m   1118\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_amp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1522\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1523\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1554\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1556\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1557\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1558\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mcross_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0mencoder_last_hidden_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m             \u001b[0mencoder_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Seq2SeqModelOutput' object has no attribute 'hidden_states'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TL8MPxaJ3IgX"
      },
      "source": [
        "# Evaluation\n",
        "tokenizer = transformers.BertTokenizer.from_pretrained(\n",
        "    tokenizer_name\n",
        ")\n",
        "\n",
        "tf2tf = transformers.EncoderDecoderModel.from_pretrained(\n",
        "    path_checkpoint + \"/checkpoint-50000\"\n",
        ")\n",
        "\n",
        "train_data, val_data, test_data = load_data(\n",
        "    language=language,\n",
        "    ratio_corpus_wiki=ratio_corpus_wiki,\n",
        "    ratio_corpus_news=ratio_corpus_news\n",
        ")\n",
        "\n",
        "test_cuda()\n",
        "explore_corpus(train_data)\n",
        "empty_cache()\n",
        "configure_model(tf2tf, tokenizer)\n",
        "rouge = datasets.load_metric(\"rouge\")\n",
        "\n",
        "\n",
        "def generate_summary(batch):\n",
        "    inputs = tokenizer(\n",
        "        batch[\"article\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    input_ids = inputs.input_ids.to(\"cuda\")\n",
        "    attention_mask = inputs.attention_mask.to(\"cuda\")\n",
        "\n",
        "    outputs = tf2tf.generate(input_ids, attention_mask=attention_mask)\n",
        "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    batch[\"pred_summary\"] = output_str\n",
        "\n",
        "    return batch\n",
        "\n",
        "\n",
        "results = test_data.map(\n",
        "    generate_summary,\n",
        "    batched=True,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "print(\n",
        "    rouge.compute(\n",
        "        predictions=results[\"pred_summary\"],\n",
        "        references=results[\"highlights\"],\n",
        "        rouge_types=[\"rouge2\"]\n",
        "    )[\"rouge2\"].mid\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6mQqYfw3Rv8"
      },
      "source": [
        "# Example\n",
        "tokenizer = transformers.XLMRobertaTokenizer.from_pretrained(\n",
        "    tokenizer_name\n",
        ")\n",
        "\n",
        "tf2tf = transformers.EncoderDecoderModel.from_pretrained(\n",
        "    path_checkpoint\n",
        ")\n",
        "\n",
        "tf2tf.to(\"cuda\")\n",
        "text = None\n",
        "parts = []\n",
        "\n",
        "\n",
        "def split_long_texts(text):\n",
        "    limit = 512\n",
        "\n",
        "    if len(text) > limit:\n",
        "        end_index = max([\n",
        "            text.rfind(\".\", 0, limit),\n",
        "            text.rfind(\"!\", 0, limit),\n",
        "            text.rfind(\"?\", 0, limit)\n",
        "        ])\n",
        "\n",
        "        parts.append(text[0:end_index + 1].strip())\n",
        "        text = text[end_index + 1:len(text)].strip()\n",
        "        split_long_texts(text)\n",
        "\n",
        "    else:\n",
        "        parts.append(text)\n",
        "\n",
        "\n",
        "text = text_english if language == \"english\" else text_german\n",
        "split_long_texts(text)\n",
        "\n",
        "if len(parts) > 1:\n",
        "    temp = {\n",
        "        \"article\": parts,\n",
        "        \"highlights\": [\"Zusammenfassung\"] * len(parts)\n",
        "    }\n",
        "\n",
        "else:\n",
        "    temp = {\n",
        "        \"article\": [text, text],\n",
        "        \"hightlights\": [\"Zusammenfassung\", \"Zusammenfassung\"]\n",
        "    }\n",
        "\n",
        "test_cuda()\n",
        "empty_cache()\n",
        "rouge = datasets.load_metric(\"rouge\")\n",
        "\n",
        "test_data = datasets.arrow_dataset.Dataset.from_pandas(\n",
        "    pd.DataFrame.from_dict(\n",
        "        temp, columns=[\"article\", \"highlights\"], orient=\"index\"\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "def generate_summary(batch):\n",
        "    inputs = tokenizer(\n",
        "        batch[\"article\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    input_ids = inputs.input_ids.to(\"cuda\")\n",
        "    attention_mask = inputs.attention_mask.to(\"cuda\")\n",
        "\n",
        "    outputs = tf2tf.generate(input_ids, attention_mask=attention_mask)\n",
        "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    batch[\"pred_summary\"] = output_str\n",
        "\n",
        "    return batch\n",
        "\n",
        "\n",
        "summary = test_data.map(\n",
        "    generate_summary,\n",
        "    batched=True,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "for i in range(0, len(parts) - 1):\n",
        "    print(f\"HYP: {summary[i]['pred_summary']}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}